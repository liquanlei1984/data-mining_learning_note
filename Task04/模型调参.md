# **模型调参**

图片已更新

**内容详情：**

1. 线性回归模型：

- - 线性回归对于特征的要求；
  - 处理长尾分布；
  - 理解线性回归模型；

1. 模型性能验证：

- - 评价函数与目标函数；
  - 交叉验证方法；
  - 留一验证方法；
  - 针对时间序列问题的验证；
  - 绘制学习率曲线；
  - 绘制验证曲线；

1. 嵌入式特征选择：

- - Lasso回归；
  - Ridge回归；
  - 决策树；

1. 模型对比：

- - 常用线性模型；
  - 常用非线性模型；

1. 模型调参：

- - 贪心调参方法；
  - 网格调参方法；
  - 贝叶斯调参方法；

**相关原理介绍与推荐**

由于相关算法原理篇幅较长，本文推荐了一些博客与教材供初学者们进行学习。

**1 线性回归模型**

https://zhuanlan.zhihu.com/p/49480391

**2 决策树模型**

https://zhuanlan.zhihu.com/p/65304798

**3 GBDT模型**

https://zhuanlan.zhihu.com/p/45145899

**4 XGBoost模型**

https://zhuanlan.zhihu.com/p/86816771

**5 LightGBM模型**

https://zhuanlan.zhihu.com/p/89360721

**6 推荐教材：**

- 《机器学习》 https://book.douban.com/subject/26708119/
- 《统计学习方法》 https://book.douban.com/subject/10590856/
- 《Python大战机器学习》 https://book.douban.com/subject/26987890/
- 《面向机器学习的特征工程》 https://book.douban.com/subject/26826639/
- 《数据科学家访谈录》 https://book.douban.com/subject/30129410/

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/01.png)

<!-- TOC -->
- [**模型调参**](#模型调参)   
  - [**一、线性回归模型**](#一线性回归模型)   
  - [**二、模型性能验证**](#二模型性能验证)
  - [**三、嵌入式特征选择**](#三嵌入式特征选择)    
  - [**四、模型对比**](#四模型对比)    
  - [**五、模型调参**](#五模型调参)
  <!-- /TOC -->

## **一、线性回归模型**

线性回归是一种回归分析技术，回归分析本质上就是一个函数估计的问题（函数估计包括参数估计和非参数估计），就是找出因变量和自变量之间的因果关系。回归分析的因变量是应该是连续变量，若因变量为离散变量，则问题转化为分类问题，回归分析是一个有监督学习问题。

线性其实就是一系列一次特征的线性组合，在二维空间中是一条直线，在三维空间中是一个平面，然后推广到n维空间，可以理解维广义线性吧。

例如对房屋的价格预测，首先提取特征，特征的选取会影响模型的精度，比如房屋的高度与房屋的面积，毫无疑问面积是影响房价的重要因素，二高度基本与房价不相关

下图中挑选了 面积、我是数量、层数、建成时间四个特征，然后选取了一些train Set{x(i) , y(i)}。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/02.png)

 有了这些数据之后就是进行训练，下面附一张有监督学习的示意图

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/03.png)

Train Set 根据 学习算法得到模型h，对New Data x，直接用模型即可得到预测值y，本例中即可得到房屋大小，其实本质上就是根据历史数据来发现规律，事情总是偏向于向历史发生过次数多的方向发展。

下面就是计算模型了，才去的措施是经验风险最小化，即我们训练模型的宗旨是，模型训练数据上产生结果

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/04.png)

, 要与实际的y(i)越接近越好

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/05.png)

（假定x0 =1），定义损失函数J(θ)如下,即我们需要损失函数越小越好，本方法定义的J(θ)在最优化理论中称为凸（Convex）函数，即全局只有一个最优解，然后通过梯度下降算法找到最优解即可，梯度下降的形式已经给出。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/06.png)

梯度下降的具体形式：关于梯度下降的细节，请参阅 [梯度下降详解](http://www.cnblogs.com/ooon/p/4947688.html)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/07.png)

**局部加权回归**

有时候样本的波动很明显，可以采用局部加权回归，如下图，红色的线为局部加权回归的结果，蓝色的线为普通的多项式回归的结果。蓝色的线有一些欠拟合了。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/08.png)

局部加权回归的方法如下，首先看线性或多项式回归的损失函数“

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/09.png)

很明显，局部加权回归在每一次预测新样本时都会重新确定参数，以达到更好的预测效果。当数据规模比较大的时候计算量很大，学习效率很低。并且局部加权回归也不是一定就是避免**underfitting，**因为那些波动的样本可能是异常值或者数据噪声。

在求解线性回归的模型时，有两个需要注意的问题

一就**是特征组合**问题，比如房子的长和宽作为两个特征参与模型的构造，不如把其相乘得到面积然后作为一个特征来进行求解，这样在特征选择上就做了减少维度的工作。

二就是**特征归一化**（Feature Scaling），这也是许多机器学习模型都需要注意的问题。

有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则**必须**进行标准化，以免模型参数被分布范围较大或较小的数据dominate。

有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，**最好**也进行数据标准化。

归一化后有两个好处：

\1. 提升模型的收敛速度

如下图，x1的取值为0-2000，而x2的取值为1-5，假如只有这两个特征，对其进行优化时，会得到一个窄长的椭圆形，导致在梯度下降时，梯度的方向为垂直等高线的方向而走之字形路线，这样会使迭代很慢，相比之下，右图的迭代就会很快

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/10.png)

2.提升模型的精度

归一化的另一好处是提高精度，这在涉及到一些距离计算的算法时效果显著，比如算法要计算欧氏距离，上图中x2的取值范围比较小，涉及到距离计算时其对结果的影响远比x1带来的小，所以这就会造成精度的损失。所以归一化很有必要，他可以让各个特征对结果做出的贡献相同。

下边是常用归一化方法

1）. 线性归一化，线性归一化会把输入数据都转换到[0 1]的范围，公式如下

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/11.png)

该方法实现对原始数据的等比例缩放，其中Xnorm为归一化后的数据，X为原始数据，Xmax、Xmin分别为原始数据集的最大值和最小值。

2）. 0均值标准化，0均值归一化方法将原始数据集归一化为均值为0、方差1的数据集，归一化公式如下：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/12.png)

其中，μ、σ分别为原始数据集的均值和方法。该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。

关于归一化方法的选择

1） 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。

2） 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。

https://blog.csdn.net/magicchu/article/details/51767409

**处理长尾特征**

**0x01 统计变换**

数据分布的倾斜有很多负面的影响。我们可以使用特征工程技巧，利用统计或数学变换来减轻数据分布倾斜的影响。使原本密集的区间的值尽可能的分散，原本分散的区间的值尽量的聚合。

这些变换函数都属于幂变换函数簇，通常用来创建单调的数据变换。它们的主要作用在于它能帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。

**1.Log变换**

1）定义

Log变换通常用来创建单调的数据变换。它的主要作用在于帮助稳定方差，始终保持分布接近于正态分布并使得数据与分布的平均值无关。

Log 变换属于幂变换函数簇。该函数用数学表达式表示为

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/13.png)

自然对数使用 b=e，e=2.71828，通常叫作欧拉常数。你可以使用通常在十进制系统中使用的 b=10 作为底数。

**当应用于倾斜分布时 Log 变换是很有用的，因为Log变换倾向于拉伸那些落在较低的幅度范围内自变量值的范围，倾向于压缩或减少更高幅度范围内的自变量值的范围。从而使得倾斜分布尽可能的接近正态分布。** 

2）作用

针对一些数值连续特征的方差不稳定，特征值重尾分布我们需要采用Log化来调整整个数据分布的方差，属于方差稳定型数据转换。比如在词频统计中，有些介词的出现数量远远高于其他词，这种词频分布的特征就会现有些词频特征值极不协调的状况，拉大了整个数据分布的方差。这个时候，可以考虑Log化。尤其在分本分析领域，时间序列分析领域，Log化非常常见, 其目标是让方差稳定，把目标关注在其波动之上。

3）变换效果

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/14.png)

4）实现代码
```python
fcc_survey_df['Income_log'] = np.log((1+fcc_survey_df['Income']))
```
**2.Box-Cox变换**

1）定义

Box-Cox 变换是另一个流行的幂变换函数簇中的一个函数。该函数有一个前提条件，即数值型值必须先变换为正数（与 log 变换所要求的一样）。万一出现数值是负的，使用一个常数对数值进行偏移是有帮助的。

Box-Cox 变换函数：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/15.png)

生成的变换后的输出y是输入 x 和变换参数的函数；当 λ=0 时，该变换就是自然对数 log 变换，前面我们已经提到过了。λ 的最佳取值通常由最大似然或最大对数似然确定。

2）作用

Box-Cox变换是Box和Cox在1964年提出的一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。Box-Cox变换之后，可以一定程度上减小不可观测的误差和预测变量的相关性。Box-Cox变换的主要特点是引入一个参数，通过数据本身估计该参数进而确定应采取的数据变换形式，Box-Cox变换可以明显地改善数据的正态性、对称性和方差相等性，对许多实际数据都是行之有效的。

3）变化效果

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/16.png)

 

4）实现代码
```python
import scipy.stats as spstats

\# 从数据分布中移除非零值

income = np.array(fcc_survey_df['Income'])

income_clean = income[~np.isnan(income)]

\# 计算最佳λ值

l, opt_lambda = spstats.boxcox(income_clean)

print('Optimal lambda value:', opt_lambda)
```

**0x02 分类特征（类别特征）编码**

\# 进行Box-Cox变换
```python
fcc_survey_df['Income_boxcox_lambda_opt'] = spstats.boxcox(fcc_survey_df['Income'],lmbda=opt_lambda)
```
在统计学中，分类特征是可以采用有限且通常固定数量的可能值之一的变量，基于某些定性属性将每个个体或其他观察单元分配给特定组或名义类别。

**1.标签编码（LabelEncode）**

1）定义

LabelEncoder是对不连续的数字或者文本进行编号，编码值介于0和n_classes-1之间的标签。

2）优缺点

**优点：**相对于OneHot编码，LabelEncoder编码占用内存空间小，并且支持文本特征编码。

**缺点：**它隐含了一个假设：不同的类别之间，存在一种顺序关系。在具体的代码实现里，LabelEncoder会对定性特征列中的所有独特数据进行一次排序，从而得出从原始输入到整数的映射。所以目前还没有发现标签编码的广泛使用，一般在树模型中可以使用。

例如：比如有[dog,cat,dog,mouse,cat]，我们把其转换为[1,2,1,3,2]。这里就产生了一个奇怪的现象：dog和mouse的平均值是cat。

3）实现代码
```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

le.fit(["paris", "paris", "tokyo", "amsterdam"])

print('特征：{}'.format(list(le.classes_)))

\# 输出 特征：['amsterdam', 'paris', 'tokyo']

print('转换标签值：{}'.format(le.transform(["tokyo", "tokyo", "paris"])))

\# 输出 转换标签值：array([2, 2, 1]...)

print('特征标签值反转：{}'.format(list(le.inverse_transform([2, 2, 1]))))

\# 输出 特征标签值反转：['tokyo', 'tokyo', 'paris']
```
**2.独热编码（OneHotEncode）**

1）定义

OneHotEncoder用于将表示分类的数据扩维。最简单的理解就是与位图类似，设置一个个数与类型数量相同的全0数组，每一位对应一个类型，如该位为1，该数字表示该类型。

**OneHotEncode只能对数值型变量二值化，无法直接对字符串型的类别变量编码。**

2）为什么要使用独热编码

独热编码是因为大部分算法是基于向量空间中的度量来进行计算的，为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。**将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。** 

**为什么特征向量要映射到欧式空间？**

将离散特征通过one-hot编码映射到欧式空间，是因为，在回归、分类、聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算。

3）例子

假如有三种颜色特征：红、黄、蓝。

在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3。那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红<黄<蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。

所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。

4）优缺点

**优点：**独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。

**缺点：**当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。

5）实现代码

使用sklearn实现

注：当特征是字符串类型时，需要先用 LabelEncoder() 转换成连续的数值型变量，再用 OneHotEncoder() 二值化 。
```python
from sklearn.preprocessing import OneHotEncoder

enc = OneHotEncoder()

enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])    # fit来学习编码

enc.transform([[0, 1, 3]]).toarray()    # 进行编码

\# 输出：array([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]])

使用pandas实现

import pandas as pd

import numpy as np

sex_list = ['MALE', 'FEMALE', np.NaN, 'FEMALE', ]

df = pd.DataFrame({'SEX': sex_list})

display(df)

\# 输出

​    SEX

0    MALE

1    FEMALE

2    NaN

3    FEMALE

df = pd.get_dummies(df['SEX'],prefix='IS_SEX')

display(df)

\# 输出

​    IS_SEX_FEMALE    IS_SEX_MALE

0    0               1

1    1               0

2    0               0

3    1               0
```
**3.标签二值化（LabelBinarizer）**

1）定义

功能与OneHotEncoder一样，但是OneHotEncode只能对数值型变量二值化，无法直接对字符串型的类别变量编码，而LabelBinarizer可以直接对字符型变量二值化。

2）实现代码
```python
from sklearn.preprocessing import LabelBinarizer

lb = LabelBinarizer()

lb.fit([1, 2, 6, 4, 2])

print(lb.classes_)

\# 输出 array([1, 2, 4, 6])

print(lb.transform([1, 6]))

\# 输出 array([[1, 0, 0, 0],

​             [0, 0, 0, 1]])

print(lb.fit_transform(['yes', 'no', 'no', 'yes']))

\# 输出 array([[1],

​             [0],

​             [0],

​             [1]])
```
**4.多标签二值化（MultiLabelBinarizer）**

1）定义

用于label encoding，生成一个(n_examples * n_classes)大小的0~1矩阵，每个样本可能对应多个label。

2）适用情况

- 每个特征中有多个文本单词；

 用户兴趣特征（如特征值：

”健身 电影 音乐”）适合使用多标签二值化，因为每个用户可以同时存在多种兴趣爱好。

- 多分类类别值编码的情况。

 电影分类标签中（如：

[action, horror]和[romance, commedy]）需要先进行多标签二值化，然后使用二值化后的值作为训练数据的标签值。

3）实现代码
```python
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()

print(mlb.fit_transform([(1, 2), (3,)]))

\# 输出

array([[1, 1, 0],

​       [0, 0, 1]])

print(mlb.classes_)

\# 输出：array([1, 2, 3])

print(mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}]))

\# 输出：array([[0, 1, 1],

​       [1, 0, 0]])

print(list(mlb.classes_))

\# 输出：['comedy', 'sci-fi', 'thriller']
```
**5.平均数编码（Mean Encoding）**

1）定义

平均数编码（mean encoding），针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。

平均数编码（mean encoding）的编码方法，在贝叶斯的架构下，利用所要预测的应变量（target variable），有监督地确定最适合这个定性特征的编码方式。在Kaggle的数据竞赛中，这也是一种常见的提高分数的手段。

算法原理详情可参考：平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程。

2）为什么要用平均数编码

如果某一个特征是定性的（categorical），而这个特征的可能值非常多（高基数），那么平均数编码（mean encoding）是一种高效的编码方式。在实际应用中，这类特征工程能极大提升模型的性能。

因为定性特征表示某个数据属于一个特定的类别，所以在数值上，定性特征值通常是从0到n的离散整数。例子：花瓣的颜色（红、黄、蓝）、性别（男、女）、地址、某一列特征是否存在缺失值（这种NA 指示列常常会提供有效的额外信息）。

一般情况下，针对定性特征，我们只需要使用sklearn的OneHotEncoder或LabelEncoder进行编码，这类简单的预处理能够满足大多数数据挖掘算法的需求。定性特征的基数（cardinality）指的是这个定性特征所有可能的不同值的数量。在高基数（high cardinality）的定性特征面前，这些数据预处理的方法往往得不到令人满意的结果。

3）优点

和独热编码相比，节省内存、减少算法计算时间、有效增强模型表现。

4）实现代码
```python
MeanEnocodeFeature = ['item_city_id','item_brand_id'] #声明需要平均数编码的特征

ME = MeanEncoder(MeanEnocodeFeature) #声明平均数编码的类

trans_train = ME.fit_transform(X,y)#对训练数据集的X和y进行拟合

test_trans = ME.transform(X_test)#对测试集进行编码
```
MeanEncoder实现源码详情可参考：平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程。

**0x0FF 总结**

1. 特征预处理是数据预处理过程的重要步骤，是对数据的一个的标准的处理，几乎所有的数据处理过程都会涉及该步骤。
2. 由于树模型（Random Forest、GBDT、xgboost等）对特征数值幅度不敏感，可以不进行无量纲化和统计变换处理；

同时，由于树模型依赖于样本距离来进行学习，所以也可以不进行类别特征编码（但字符型特征不能直接作为输入，所以需要至少要进行标签编码）。

1. 依赖样本距离来学习的模型（如线性回归、SVM、深度学习等）

- - 对于数值型特征需要进行无量纲化处理；
  - 对于一些长尾分布的数据特征，可以做统计变换，使得模型能更好优化；
  - 对于线性模型，特征分箱可以提升模型表达能力；

1. 对数值型特征进行特征分箱可以让模型对异常数据有很强的鲁棒性，模型也会更稳定。

另外，分箱后需要进行特征编码。

**参考文献**

[1] sklearn中的数据预处理. http://d0evi1.com/sklearn/preprocessing/

[2] 归一化与标准化. https://ssjcoding.github.io/2019/03/27/normalization-and-standardization/

[3] Preprocessing Data : 類別型特徵_OneHotEncoder & LabelEncoder 介紹與實作. https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/preprocessing-data-onehotencoder-labelencoder-%E5%AF%A6%E4%BD%9C-968936124d59

[4] 平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程. https://zhuanlan.zhihu.com/p/26308272

[5] 特征工程之分箱. https://blog.csdn.net/Pylady/article/details/78882220

[6] https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html

[7] https://www.cnblogs.com/vegbirds/p/12188650.html

**理解线性回归模型**

**1. 线性回归**

**什么是回归**？

从大量的函数结果和自变量反推回函数表达式的过程就是回归。线性回归是利用数理统计中回归分析来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。

**一元线性回归：**

只包括一个自变量（x）和一个因变量（y），且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。公式： $y=ax+b$

**多元线性回归：**

如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。公式：

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/18.png)

对于一元线性回归公式来说，如果给定两组x,y的值，就可以求得和的值。例如：给定两组x，y的值x=3,y=5和x=6,y=8，那么可知如下：

5=+*3         8=+*6

可以求得=2,=1。两点确定一条直线，可以确定和的值。我们可以叫做一元线性回归公式的**截距**，可以叫做一元线性回归公式的**斜率**。

假设某个医院住院人数与医院的床位个数呈线性关系，即：

​     住院人数 = +*医院床位数

在这里，如果想求得住院的人数，就要知道，的值，这里的，也叫**权重**,“医院床位数”可以理解成影响住院人数的一个**维度**。通过给出两组数据，就可以得到权重的值。

对于一元线性回归来说，如果现在有很多呈线性关系离散的点，假设现在需要找到一个线性回归公式来表示这些点自变量和因变量的关系，每两个点之间都能确定一条直线，如何找到一条直线来表示这些点的关系呢？

**如何确定这条直线？**

每一个离散的点在自变量处得到的的值记为，对应的，需要找到的这条直线在自变量处得到的的值记为，当所有的到对应的的距离差平方累加起来最小，我们就可以认为这条直线比较完美。也就是当平均误差公式：error= 最小时这条直线比较完美，其中，

代表一系列的真实离散点的y值，

代表一系列得到的直线在对应的x处的y值，一元线性回归中=。

m代表有m个离散点，前面的 是为了方便求导加上的。

 表示的是一系列权重,,…. 

这个公式也叫做**最小二乘法误差公式**。error的值如果是0说明确定的这条直线穿过了所有的点，对于离散分散的一组点，error的值不可能为0。

 

**2. 确定权重的值（确定模型）**

当误差公式:

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/15.png)

 

取得最小值时，能确定一条完美的直线，也就是确定一组 值与自变量的关系，拟合出来的一条直线能尽可能的通过更多的点。确定了error的最小值，也就有可能确定一组值。

**如何确定error的最小值？**

如果将error函数看成自变量关于error的函数，那么可以转换为：

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/16.png)

 

如果将一组权重看成一列向量，表示一列维度组成的向量，就是这组向量的转置，

就是，那么现在的问题就是如何找到一组，使得error的值最小。

我们可以看出error函数的关系图像大概是一个开口向上的图像，那么当error的导数为0时，此时能得到一组的值对应的error的值最小。

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/17.png)

 

通过求导的方式，理论上可以确定error的最小值，但是由于是一组数据，无法确定error最小下对应的这一组权重 。

正向的求导不能得到一组权重值，就不能确定线性回归的公式。那么可以根据数据集来穷举法反向的试，来确定线性回归的公式：

**如何反向穷举求得线性回归的公式（训练线性回归模型）？**

已知有线性回归关系的训练数据

已知线性回归的公式：

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/18.png)

 

已知误差函数公式：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/19.png)

 

要求得在error最小下的一组权重值，以便确定线性回归的公式。我们要确定的这组就是要确定一个模型。确定模型就是确定一组参数，就是确定这组的值。

穷举之前，会有一个初始模型，一般是随机数，假设=1，=1，=1…,有了初始模型那么针对每一条训练数据都能求得一个真实值与模型评估值的平方差，也就是针对一组测试数据，能得到在这组初始模型下error误差函数的值。那么得到的error值是不是最小值？需要不断调节初始模型参数，需要在训练模型时，指定调节初始模型，，…的调节步长step（也叫学习率）,迭代去求出每次调节模型后的error值，由于error的值不可能绝对的达到0（如果达到0说明训练出来的模型对应的线性回归公式完全的将所有训练数据集的点穿过），在训练模型之前指定一个最小error值，即：当迭代求出模型的error误差值小于指定的error值就停止迭代，那么当前这组值就是确定好的模型。

如果每次迭代求得模型对应的error大于指定的最小error值，要继续调节这组值，一直迭代到模型的error值小于指定的error值。

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/20.png)

 

 

**3. 梯度下降法调节参数**

假设和error的函数关系中，中只有一个维度，那么这个关系其实就是和error的关系，反应图上就是一维平面关系，如果中有两个维度和，那么这个反映到图上就是个三维空间关系，以此类推。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/21.png)

如果中只有一个维度，如下图：

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/22.png)

 

 

 

图中①，②，③，④，⑤，⑥，⑦都是图像的切线（斜率），在切点处可以求得对应的error值，如何调节模型得到一组 值可以使得到的error值更小，如图，沿着①->②->③方向调节和沿着④->⑤->⑥方向调节，可以使error值更小，这种沿着斜率绝对值减少的方向,沿着梯度的负方向每次迭代调节的方法就叫做**梯度下降法**。

梯度下降是迭代法的一种,可以用于求解最小二乘问题，在求解error函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的error函数和模型参数值。

每次按照步长(学习率)来调节值，迭代求出error的最小值，这里的error不可能有一个固定的最小值，只会向着最小值的方向收敛。

判断模型error误差值收敛，也就是停止迭代的两种方式：

1. 指定一个error值，当迭代处理过程中得到的error值小于指定的值时，停止迭代。
2. 设置迭代次数，当迭代次数达到设置的次数时，停止迭代。

迭代周期：从调整参数开始到计算出error值判断是否大于用户指定的error是一个迭代周期。

当步长比较小时，迭代次数多，训练模型的时间比较长，当步长比较大时，有可能迭代的次数也比较多，训练模型的时间也会相对比较长，需要找到一个合适的步长。

https://www.cnblogs.com/eric666666/p/11312048.html

## **二、模型性能验证**

**评价函数与目标函数**

衡量分类器的好坏

​    对于二类分类器/分类算法，评价指标主要有accuracy， [precision，recall，F-score，pr曲线]，ROC-AUC曲线，gini系数。

​    对于多类分类器/分类算法，评价指标主要有accuracy， [宏平均和微平均，F-score]。

​    对于回归分析，主要有mse和r2/拟合优度。

二分类模型的评估

机器学习系统设计系统评估标准

Error Metrics for Skewed Classes有偏类的错误度量精确度召回率

PrecisionRecall精确度召回率

Trading Off Precision and Recall权衡精度和召回率F1值

A way to choose this threshold automatically How do we decide which of these algorithms is best

Data For Machine Learning数据影响机器学习算法的表现

[Machine Learning - XI. Machine Learning System Design机器学习系统设计(Week 6)系统评估标准 ]

召回率、准确率、F值

对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为：

真正例（True Positive，TP）：真实类别为正例，预测类别为正例。

假正例（False Positive，FP）：真实类别为负例，预测类别为正例。

假负例（False Negative，FN）：真实类别为正例，预测类别为负例。

真负例（True Negative，TN）：真实类别为负例，预测类别为负例。

然后可以构建混淆矩阵（Confusion Matrix）如下表所示。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/23.png)

**准确率，又称查准率（Precision，P）：**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/24.png)

**召回率，又称查全率（Recall，R）：**

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/25.jpeg)

 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/26.png)

G-Mean指标，也能评价不平衡数据的模型表现。

ROC-AUC曲线和PRC曲线

[分类模型评估之ROC-AUC曲线和PRC曲线]

KS曲线

  柯尔莫哥洛夫-斯米尔诺夫检验（Колмогоров-Смирнов检验）基于累计分布函数，用以检验两个经验分布是否不同或一个经验分布与另一个理想分布是否不同。ROC曲线是评判一个模型好坏的标准，但是最好的阈值是不能通过这个图知道的，要通过KS曲线得出。

  KS值越大，表示模型能够将正、负客户区分开的程度越大。 通常来讲，KS>0.2即表示模型有较好的预测准确性。

  绘制方式与ROC曲线略有相同，都要计算TPR和FPR。但是TPR和FPR都要做纵轴，横轴为把样本分成多少份。 KS曲线的纵轴是表示TPR和FPR的值，就是这两个值可以同时在一个纵轴上体现，横轴就是阈值，然后在两条曲线分隔最开的地方，对应的就是最好的阈值。

下图中，一条曲线是FPR，一条是TPR

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/27.png)

吉尼系数Gini coefficient

在用SAS或者其他一些统计分析软件，用来评测分类器分类效果时，常常会看到gini coefficient，gini系数通常被用来判断收入分配公平程度，具体请参阅wikipedia-基尼系数。

在ID3算法中我们常使用信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/28.png)

Gini coefficient 是指绝对公平线(line of equality)和洛伦茨曲线(Lorenz Curve)围成的面积与绝对公平线以下面积的比例，即gini coefficient = A面积 / (A面积+B面积) 。

用在评判分类模型的预测效力时，是指ROC曲线曲线和中线围成的面积与中线之上面积的比例。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/29.png)

因此Gini coefficient与AUC可以互相转换：

gini = A / (A + B) = (AUC - C) / (A + B) = (AUC -0.5) / 0.5 = 2*AUC - 1

Note: 特别值得注意的一点是，这里的AUC并不是roc_auc（y是二值的）计算里面的那个roc曲线面积，而是指x,y横纵坐标计算出来的面积（y不是二值而是连续值）。

Gini系数的计算
```python
def giniCoefficient(x, y):    '''    gini系数计算      :param x: 推测值（人口）      :param y: 实际值（财富）    '''    x = np.asarray(x)    y = np.asarray(y)    x.__add__(0)    y.__add__(0)     x = np.cumsum(x)    if x[-1] != 0:        x = x / x[-1]     y = np.cumsum(y)    if y[-1] != 0:        y = y / y[-1]     area = metrics.auc(x, y, reorder=True)    gini_cof = 1 - 2 * area     return gini_cof if math.fabs(gini_cof) > pow(math.e, -6) else 0
```
多分类模型的评估

accuracy

​     在多类分类的背景下，accuracy = (分类正确的样本个数) / (分类的所有样本个数)。

​     这样做其实看上去也挺不错的，不过可能会出现一个很严重的问题：例如某一个不透明的袋子里面装了1000台手机，其中有600台iphone6, 300台galaxy s6, 50台华为mate7,50台mx4(当然，这些信息分类器是不知道的。。。)。如果分类器只是简单的把所有的手机都预测为iphone6, 那么通过上面的公式计算的准确率accuracy为0.6，看起来还不错;可是三星，华为和小米的全部预测错了。如果再给一个袋子，里面装着600台galaxy s6, 300台mx4, 50台华为mate7,50台iphone，那这个分类器立马就爆炸了

宏平均（macro-average）和微平均（micro-average）

​     如果只有一个二分类混淆矩阵，那么用以上的指标就可以进行评价，没有什么争议，但是当我们在n个二分类混淆矩阵上要综合考察评价指标的时候就会用到宏平均和微平均。宏平均（macro-average）和微平均（micro-average）是衡量文本分类器的指标。根据Coping with the News: the machine learning way: When dealing with multiple classes there are two possible ways of averaging these measures(i.e. recall, precision, F1-measure) , namely, macro-average and micro-average. The macro-average weights equally all the classes, regardless of how many documents belong to it. The micro-average weights equally all the documents, thus favouring the performance on common classes. Different classifiers will perform different in common and rare categories. Learning algorithms are trained more often on more populated classes thus risking local over-fitting.

​     宏平均（Macro-averaging），是先对每一个类统计指标值，然后在对所有类求算术平均值。宏平均指标相对微平均指标而言受小类别的影响更大。即将n分类的评价拆成n个二分类的评价，计算每个二分类的score，n个 score的平均值即为Macro score。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/30.png)

 微平均（Micro-averaging），是对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。将n分类的评价拆成n个二分类的评价，将n个二分类评价的TP、FP、RN对应相加，计算评价准确率和召回率，由这2个准确率和召回率计算的F1 score即为Micro F1。

[多类分类性能评价之宏平均(macro-average)与微平均(micro-average)]

微平均（包括p\r\f）实际上就是前面说的accuracy，除非pred出了true没有的标签，如[微平均（Micro-averaging）]。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/31.png)

从上面的公式我们可以看到微平均并没有什么疑问，但是在计算宏平均F值时我给出了两个公式分别为公式（7）和（8）。都可以用。

[谈谈评价指标中的宏平均和微平均]

​     在测试数据集上，度量分类器对大类判别的有效性应该选择微平均，而度量分类器对小类判别的有效性则应该选择宏平均。

一般来讲，Macro F1、Micro F1高的分类效果好。Macro F1受样本数量少的类别影响大。

**示例**

假设有10个样本，它们属于A、B、C三个类别。假设这10个样本的真实类别和预测的类别分别是：

1. \> 真实：A A A C B C A B B C
2. \> 预测：A A C B A C A C B C

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/32.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/33.png)

Kappa一致性系数

​    交叉表（混淆矩阵）虽然比较粗糙，却是描述栅格数据随时间的变化以及变化方向的很好的方法。但是交叉表却不能从统计意义上描述变化的程度，需要一种能够测度名义变量变化的统计方法即KAPPA指数——KIA。 kappa系数是一种衡量分类精度的指标。KIA主要应用于比较分析两幅地图或图像的差异性是“偶然”因素还是“必然”因素所引起的，还经常用于检查卫星影像分类对于真实地物判断的正确性程度。KIA是能够计算整体一致性和分类一致性的指数。

KIA的计算式的一般性表示为： 

　　

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/34.png)

 

总体KAPPA指数的计算式为：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/35.png)

　　式中， 

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/36.png)

　式中的p0和pc都有着明确的含义：p0被称为观测精确性或一致性单元的比例；pc被称为偶然性一致或期望的偶然一致的单元的比例。kappa计算结果为-1~1，但通常kappa是落在 0~1 间，可分为五组来表示不同级别的一致性：0.0~0.20极低的一致性(slight)、0.21~0.40一般的一致性(fair)、0.41~0.60 中等的一致性(moderate)、0.61~0.80 高度的一致性(substantial)和0.81~1几乎完全一致(almost perfect)。 

kappa指数计算的一个示例：

混淆矩阵

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/37.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/38.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/39.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/40.png)

kappa在sklearn上的实现[[sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).cohen_kappa_score[¶](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn-metrics-cohen-kappa-score)]

**多分类roc曲线**

[[多分类的ROC曲线](http://bei.dreamcykj.com/2018/08/19/ROC原理介绍及利用python实现二分类和多分类的ROC曲线 (1)/)]

**回归模型的评估**

**平均均方误差mse**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/41.png)

**R^2 (coefficient of determination)（推荐）**

regression score function.评估模型拟合的好坏。训练集估计和学到的模型产生的新数据集评估的偏离度。sklearn中Regressor默认的score方法。

拟合优度（Goodness of Fit）是指回归直线对观测值的拟合程度。度量拟合优度的统计量是可决系数（亦称确定系数）R^2。R^2最大值为1。R^2的值越接近1，说明回归直线对观测值的拟合程度越好；反之，R^2的值越小，说明回归直线对观测值的拟合程度越差。

在用线性模型拟合完数据之后，我们需要评估模型拟合的好坏情况。当然，这种评估取决于我们想要用这个模型来做什么。一种评估模型的办法是计算模型的预测能力。

在一个预测模型中，我们要预测的值称为因变量（dependent variable），而用于预测的值称为解释变量或自变量（explanatory variable或independent variable）。

通过计算模型的确定系数（coefficient of determination），也即通常所说的R^2，来评价模型的预测能力：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/42.png)

即1 - 预测模型的mse/数据本身的mse （数据本身的mse就是直接将数据label均值作为预测的mse）

**解释R2意义例子**

假设你试图去猜测一群人的体重是多少，你知道这群人的平均体重是\bar{y}。如果除此之外你对这些人一点儿都不了解，那么你最佳的策略是选择猜测他们所有人的体重都是\bar{y}。这时，估计的均方误差就是这个群体的方差var(Y):

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/43.png)

接下来，假如我告诉你这群人的身高信息，那么你就可以猜测体重大约为

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/44.png)

，在这种情况下，估计的均方误差就为Var(ε)：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/45.png)

所以，Var(ε)/Var(Y)表示的是有解释变量情况下的均方误差与没有解释变量情况下的均方误差的比值，也即不能被模型解释的均方误差占总的均方误差的比例。这样R2表示的就是能被模型解释的变异性的比例。

假如一个模型的R^2=0.64，那么我们就可以说这个模型解释了64%的变异性，或者可以更精确地说，这个模型使你预测的均方误差降低了64%。

在线性最小二乘模型中，我们可以证明确定系数和两个变量的皮尔逊相关系数存在一个非常简单的关系，即：R2=ρ2。

[[拟合优度](https://jobrest.gitbooks.io/statistical-thinking/di_9_zhang_xiang_guan_xing/97_ni_he_you_du.html) ]

拟合优度Goodness of fit

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/46.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/47.png)

**其它**

**学习目标**

{评价学习模型的不同方法}

不同的模型通过表达了不同的折中方案。近似模型根据一种性能度量可能表现很好，但根据其它度量又可能很差。为了引导学习算法的发展，必须定义学习任务的目标，并且定义能够评价不同结果 相应的度量方法。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/48.png)

密度估计

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/49.png)

**评价指标及方法**

**期望的对数似然**

由于生成分布p*是固定的，评价指标——相对熵可以转换成最大期望的对数似然。直观上就是，M~对从真实分布中采样的点赋予的概率越大，越能反映它是该分布。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/50.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/51.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/52.png)

**数据的似然**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/53.png)

**留一验证方法**

留一法  leave-one-out cross validation

留一法就是每次只留下一个样本做测试集，其它样本做训练集，如果有k个样本，则需要训练k次，测试k次。

留一发计算最繁琐，但样本利用率最高。适合于小样本的情况。

**针对时间序列问题的验证**

导论

研究时间序列主要目的：进行预测，根据已有的时间序列数据预测未来的变化。

时间序列预测关键：确定已有的时间序列的变化模式，并假定这种模式会延续到未来。

时间序列预测法的基本特点

- - - - - - 假设事物发展趋势会延伸到未来
          - 预测所依据的数据具有不规则性
          - 不考虑事物发展之间的因果关系

时间序列数据用于**描述现象随时间发展变化的特征**。

时间序列分析就其发展历史阶段和所使用的统计分析方法看：**传统的时间序列分析和现代时间序列分析。**

一、时间序列及其分解

时间序列（time series）是同一现象在不同时间上的相继观察值排列而成的序列。根据观察时间的不同，时间序列中的时间可以是可以是年份、季度、月份或其他任何时间形式。

时间序列：

（1）平稳序列（stationary series）

是基本上不存在趋势的序列，序列中的各观察值基本上在某个固定的水平上波动，在不同时间段波动程度不同，但不存在某种规律，**随机波动**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/54.png)

（2）非平稳序列（non-stationary series）

是包含趋势、季节性或周期性的序列，只含有其中一种成分，也可能是几种成分的组合。可分为：有趋势序列、有趋势和季节性序列、几种成分混合而成的复合型序列。

**趋势（trend）**：时间序列在长时期内呈现出来的某种持续上升或持续下降的变动，也称长期趋势。时间序列中的趋势可以是线性和非线性。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/55.png)

**季节性（seasonality）**：季节变动（seasonal fluctuation）,是时间序列在一年内重复出现的周期波动。销售旺季，销售淡季，旅游旺季、旅游淡季，因季节不同而发生变化。季节，不仅指一年中的四季，其实是指任何一种周期性的变化。含有季节成分的序列可能含有趋势，也可能不含有趋势。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/56.png)

**周期性（cyclicity）**：循环波动（cyclical fluctuation），是时间序列中呈现出来的围绕长期趋势的一种波浪形或振荡式波动。周期性是由商业和经济活动引起的，不同于趋势变动，不是朝着单一方向的持续运动，而是涨落相间的交替波动；不同于季节变动，季节变动有比较固定的规律，且变动周期大多为一年，循环波动则无固定规律，变动周期多在一年以上，且周期长短不一。周期性通常是由经济环境的变化引起。

除此之外，还有偶然性因素对时间序列产生影响，致使时间序列呈现出某种随机波动。时间序列除去趋势、周期性和季节性后的偶然性波动，称为随机性（random），也称不规则波动（irregular variations）。

时间序列的成分可分为4种：趋势（T）、季节性或季节变动（S）、周期性或循环波动（C）、随机性或不规则波动（I）。传统时间序列分析的一项主要内容就是把这些成分从时间序列中分离出来，并将它们之间的关系用一定的数学关系式予以表达，而后分别进行分析。按4种成分对时间序列的影响方式不同，时间序列可分解为多种模型：加法模型（additive model），乘法模型（multiplicative model）。乘法模型：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/06.gif)

二、描述性分析

1、图形描述

2、增长率分析

是对现象在不同时间的变化状况所做的描述。由于对比的基期不同，增长率有不同的计算方法。

（1）增长率（growth rate）：增长速度，是时间序列中报告期观察值与基期观察值之比减1后的结果，用%表示。由于对比的基期不同，可分为环比增长率和定基增长率。

环比增长率：是报告期观察值与前一时期观察值之比减1，说明现象逐期增长变化的程度；

定基增长率是报告期观察值与某一固定时期观察值之比减1，说明现象在整个观察期内总的增长变化程度。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/57.png)

（2）平均增长率（average rate of increase）：平均增长速度，是时间序列中逐期环比值（环比发展速度）的几何平均数减1的结果：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/58.png)

n:环比值的个数

（3）增长率分析中应注意的问题

i:   当时间序列中的观察出现0或负数时，不宜计算增长率。这种序列计算增长率，要么不符合数学公理，要么无法解释其实际意义。可用绝对数进行分析。

ii:   有些情况下，不能单纯就增长率论增长率，注意增长率与绝对水平结合起来。增长率是一个相对值，与对比的基数值的大小有关。这种情况，计算增长1%的绝对值来克服增长率分析的局限性：

增长1%的绝对值表示增长率每增长一个百分点而增加的绝对数量：增长1%的绝对值=前期水平/100

三、时间序列预测的程序

时间序列分析的主要目的之一是根据已有的历史数据对未来进行预测。时间序列含有不同的成分，如趋势、季节性、周期性和随机性。对于一个具体的时间序列，它可能含有一种成分，也可能同时含有几种成分，含有不同成分的时间序列所用的预测方法是不同的。预测步骤：

第一步：确定时间序列所包含的成分，确定时间序列的类型

第二步：找出适合此类时间序列的预测方法

第三步：对可能的预测方法进行评估，以确定最佳预测方案

第四步：利用最佳预测方案进行预测

1、确定时间序列成分

（1）**确定趋势成分**

确定趋势成分是否存在，可绘制时间序列的线图，看时间序列是否存在趋势，以及存在趋势是线性还是非线性。

利用回归分析拟合一条趋势线，对回归系数进行显著性检验。回归系数显著，可得出线性趋势显著的结论。

（2）**确定季节成分**

确定季节成分是否存在，至少需要两年数据，且数据需要按季度、月份、周或天来记录。可绘图，年度折叠时间序列图（folded annual time series plot），需要将每年的数据分开画在图上，横轴只有一年的长度，每年的数据分别对应纵轴。如果时间序列只存在季节成分，年度折叠时间序列图中的折线将会有交叉；如果时间序列既含有季节成分又含有趋势，则年度折叠时间序列图中的折线将不会有交叉，若趋势上升，后面年度的折线将会高于前面年度的折线，若下降，则后面年度的折线将会低于前面年度的折线。

2、选择预测方法

确定时间序列类型后，选择适当的预测方法。利用时间数据进行预测，通常假定过去的变化趋势会延续到未来，这样就可以根据过去已有的形态或模式进行预测。时间序列的预测方法：传统方法：简单平均法、移动平均法、指数平滑法等，现代方法：Box-Jenkins 的自回归模型（ARMA）。

一般来说，任何时间序列都会有不规则成分存在，在商务和管理数据中通常不考虑周期性，只考虑趋势成分和季节成分。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/59.png)

不含趋势和季节成分的时间序列，即平稳时间序列只含随机成分，只要通过平滑可消除随机波动。因此，这类预测方法也称平滑预测方法。

3、预测方法的评估

在选择某种特定的方法进行预测时，需要评价该方法的预测效果或准确性。评价方法是找出预测值与实际值的差距，即预测误差。最优的预测方法就是预测误差达到最小的方法。

预测误差计算方法：平均误差，平均绝对误差、均方误差、平均百分比误差、平均绝对百分比误差。方法的选择取决于预测者的目标、对方法的熟悉程度。

（1）平均误差（mean error）:Y:观测值，F：预测值，n预测值个数

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\612cd8ac403345d7999db1b0efe9e999\01.gif)

由于预测误差的数值可能有正有负，求和的结果就会相互抵消，这种情况下，平均误差可能会低估误差。

（2）平均绝对误差（mean absolute deviation）是将预测误差取绝对值后计算的平均无擦，MAD：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\b69d1a9744ae4ef7b9be8f025e92db34\02.gif)

平均绝对误差可避免误差相互抵消的问题，因而可以准确反映实际预测误差的大小。

（3）均方误差（mean square error）:通过平方消去误差的正负号后计算的平均误差，MSE:

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\2161e16eaffa40138a9bc09cf5102914\03.gif)

（4）平均百分比误差和平均绝对百分比误差

ME,MAD,MSE的大小受时间序列数据的水平和计量单位的影响，有时并不能真正反映预测模型的好坏，只有在比较不同模型对同一数据的预测时才有意义。平均百分比误差（mean percentage error，MPE）和平均绝对百分比误差（mean absolute percentage error,MAPE）则不同，它们消除了时间序列数据的水平和计量单位的影响，是反映误差大小的相对值。

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\3909ab4641e34b11877e41e9b749fbf7\04.gif)

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\de8654d09bd8430aadcd1e8f93c52543\05.gif)

4、平稳序列的预测

平稳时间序列只含有随机成分，预测方法：简单平均法、移动平均法、指数平滑法。主要通过对时间序列进行平滑以消除随机波动，又称平滑法。平滑法可用于对时间序列进行短期预测，也可对时间序列进行平滑以描述序列的趋势（线性趋势和非线性趋势）。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/60.png)

简单平均法适合对较为平稳的时间序列进行预测，即当时间序列没有趋势时，用该方法比较好。但如果时间序列有趋势或季节成分，该方法的预测则不够准确。简单平均法将远期的数值和近期的数值看作对未来同等重要。从预测角度，近期的数值比远期的数值对未来有更大的作用，因此简单平均法预测的结果不够准确。

（2）移动平均法（moving average）：通过对时间序列逐期递移求得平均数作为预测值的一种预测方法，有简单移动平均法（simple moving average）和加权移动平均法（weighted moving average）.

简单移动平均将最近k期数据加以平均，作为下一期的预测值。设移动平均间隔为k(1<k<t),则t期的移动平均值为：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/61.png)

移动平均法只使用最近k期的数据，在每次计算移动平均值时，移动的间隔都为k,也适合对较为平稳的时间序列进行预测。应用关键是确定合理的移动平均间隔k。对于同一个时间序列，采用不同的移动间隔，预测的准确性是不同的。可通过试验的方法，选择一个使均方误差达到最小的移动间隔。移动间隔小，能快速反映变化，但不能反映变化趋势；移动间隔大，能反映变化趋势，但预测值带有明显的滞后偏差。

移动平均法的基本思想：移动平均可以消除或减少时间序列数据受偶然性因素干扰而产生的随机变动影响，适合短期预测。

（3）指数平滑法（exponential smoothing）是通过对过去的观察值加权平均进行预测，使t+1期的预测值等t期的实际观察值与t期的预测值的加权的平均值。指数平滑法是从移动平均法发展而来，是一种改良的加权平均法，在不舍弃历史数据的前提下，对离预测期较近的历史数据给予较大权数，权数由近到远按指数规律递减，因此称指数平滑。指数平滑有一次指数平滑法、二次指数平滑法、三次指数平滑法等。

一次指数平滑法也称单一指数平滑法（single exponential smoothing），只有一个平滑系数，且观察值离预测时期越久远，权数变得越小。一次指数平滑是将一段时期的预测值与观察值的线性组合作为t+1时期的预测值，预测模型为：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/62.png)

与移动平均法一样，一次指数平滑法可用于对时间序列进行修匀，以消除随机波动，找出序列的变化趋势。

用一次指数平滑法进行预测时，一般取值不大于0.5，若大于0.5，才能接近实际值，说明序列有某种趋势或波动过大。

阻尼系数β=1-α，阻尼系数越小，近期实际值对预测结果的影响越大，反之，越小。阻尼系数是根据时间序列的变化特性来选取。

5、趋势型序列的预测

时间序列的趋势可分为线性趋势和非线性趋势，若这种趋势能够延续到未来，就可利用趋势进行外推预测。有趋势序列的预测方法主要有线性趋势预测、非线性趋势预测和自回归模型预测。

（1） 线性趋势预测

线性趋势（linear trend）是指现象随着时间的推移而呈现稳定增长或下降的线性变化规律。

趋势方程：   :时间序列的预测值；是趋势线斜率，表示时间t 变动一个单位，观察值的平均变动数量

（2） 非线性趋势预测

序列中的趋势通常可认为是由于某种固定因素作用同一方向所形成的。若这种因素随时间推移按线性变化，则可对时间序列拟合趋势直线；若呈现出某种非线性趋势（non-linear trend）,则需要拟合适当的趋势曲线。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/63.png)

ii:  多阶曲线:

有些现象变化形态复杂，不是按照某种固定的形态变化，而是有升有降，在变化过程中可能有几个拐点。这时就需要拟合多项式函数。当只有一个拐点时，可拟合二项曲线，即抛物线；当有两个拐点时，需要拟合三阶曲线；有k-1个拐点时，需要拟合k阶曲线。

**6、复合型序列的分解预测**

复合型序列是指含有趋势、季节、周期和随机成分的序列。对这类序列的预测方法是将时间序列的各个因素依次分解出来，然后进行预测。由于周期成分的分析需要有多年的数据，实际中很难得到多年的数据，因此采用的分解模型为：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\0adf011bbd18454785b5ee6357cad05e\06.gif)

预测方法有：季节性多元回归模型、季节自回归模型和时间序列分解法预测。

分解法预测步骤：

第一步：确定并分离季节成分。计算季节指数，以确定时间序列中的季节成分。然后将季节成分从时间序列中分离出去，即用每一个时间序列观察值除以相应的季节指数，以消除季节性。

第二步：建立预测模型并进行预测。对消除了季节成分的时间序列建立适当的预测模型，并根据这一模型进行预测。

第三步：计算最后的预测值。用预测值乘以相应的季节指数，得到最终的预测值。

（1）确定并分离季节成分

季节性因素分析是通过季节指数来表示各年的季节成分，以此描述各年的季节变动模式。

**i:  计算季节指数（seasonal index）**

季节指数刻画了序列在一个年度内各月或各季度的典型季节特征。在乘法模型中，季节指数以其平均数等于100%为条件而构成的，反映了某一月份或季度的数值占全年平均值的大小。若现象的发展没有季节变动，则各期的季节指数应等于100%；若某一月份或季度有明显的季节变化，则各期的季节指数应大于或小于100%。因此，季节变动的程度是根据各季节指数与其平均数（100%）的偏差程度来测定的。

季节指数计算方法较多，移动平均趋势剔除法步骤：

第一步：计算移动平均值（若是季节数据，采用4项移动平均，月份数据则采用12项移动平均），并对其结果进行中心化处理，即将移动平均的结果再进行一次二项移动平均，即得出中心化移动平均值（CMA）。

第二步：计算移动平均的比值，即季节比率，即将序列的各观察值除以相应的中心化移动平均值，然后计算出各比值的季度或月份平均值。

第三步：季节指数调整。由于各季节指数的平均数应应等于1或100%，若根据第二步计算的季节比率的平均值不等于1，则需要进行调整。具体方法：将第二步计算的每个季节比率的平均值除以它们的总平均值。

ii:  分离季节成分

计算出季节指数后，可将各实际观察值分别除以相应的季节指数，将季节成分从时间序列中分离出去：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\74e547309deb4cff9599382a1ef28610\07.gif)

结果即为季节成分分离后的序列，反映了在没有季节因素影响下时间序列的变化形态。

iii: 建立预测模型并进行预测

7、时序案例分析

https://blog.csdn.net/mengjizhiyou/article/details/104765862

**绘制学习率曲线**

**学习曲线：**一种用来判断训练模型的一种方法，通过观察绘制出来的学习曲线图，我们可以比较直观的了解到我们的模型处于一个什么样的状态，如：过拟合（overfitting）或欠拟合（underfitting）

**先来看看如何解析学习曲线图：**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/64.png)

要看深刻了解上面的图形意义，你需要了解偏差（bias）、方差（variance）对于训练模型的意义，可以参考这里，当你了解后，我们来看看上面的图形代表的意义：（横坐标表示训练样本的数量，纵坐标表示准确率）

  1：观察左上图，训练集准确率与验证集准确率收敛，但是两者收敛后的准确率远小于我们的期望准确率（上面那条红线），所以由图可得该模型属于欠拟合（underfitting）问题。由于欠拟合，所以我们需要增加模型的复杂度，比如，增加特征、增加树的深度、减小正则项等等，此时再增加数据量是不起作用的。

  2：观察右上图，训练集准确率高于期望值，验证集则低于期望值，两者之间有很大的间距，误差很大，对于新的数据集模型适应性较差，所以由图可得该模型属于过拟合（overfitting）问题。由于过拟合，所以我们降低模型的复杂度，比如减小树的深度、增大分裂节点样本数、增大样本数、减少特征数等等。

  3：一个比较理想的学习曲线图应当是：低偏差、低方差，即收敛且误差小。
```python
import numpy as np import matplotlib.pyplot as plt from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit   def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):    plt.figure()    plt.title(title)    if ylim is not None:        plt.ylim(*ylim)    plt.xlabel("Training examples")    plt.ylabel("Score")    train_sizes, train_scores, test_scores = learning_curve(        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)    train_scores_mean = np.mean(train_scores, axis=1)    train_scores_std = np.std(train_scores, axis=1)    test_scores_mean = np.mean(test_scores, axis=1)    test_scores_std = np.std(test_scores, axis=1)    plt.grid()     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,                     train_scores_mean + train_scores_std, alpha=0.1,                     color="r")    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,                     test_scores_mean + test_scores_std, alpha=0.1, color="g")    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",             label="Training score")    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",             label="Cross-validation score")     plt.legend(loc="best")    return plt   digits = load_digits() X, y = digits.data, digits.target    # 加载样例数据  # 图一 title = r"Learning Curves (Naive Bayes)" cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0) estimator = GaussianNB()    #建模 plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=1)  # 图二 title = r"Learning Curves (SVM, RBF kernel, $\gamma=0.001$)" cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0) estimator = SVC(gamma=0.001)    # 建模 plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=1)  plt.show()
```
**plot_learning_curve函数官方放提供的模板函数，可以无需修改，初学时我们仅需要知道传入的参数意义即可。**

先说说函数里面的一个东西，也是画曲线的核心sklearn.model_selection的learning_curve，该学习曲线函数返回的是train_sizes，train_scores，test_scores：

  在画训练集的曲线时：横轴为 train_sizes，纵轴为 train_scores_mean；

  画测试集的曲线时：横轴为train_sizes，纵轴为test_scores_mean。

**title：**图像的名字。

**cv：**默认cv=None，如果需要传入则如下：

  cv : int, 交叉验证生成器或可迭代的可选项，确定交叉验证拆分策略。

​     cv的可能输入是：

​      \- 无，使用默认的3倍交叉验证，

​      \- 整数，指定折叠数。

​      \- 要用作交叉验证生成器的对象。

​      \- 可迭代的yielding训练/测试分裂。

**ShuffleSplit：**我们这里设置cv，交叉验证使用ShuffleSplit方法，一共取得100组训练集与测试集，每次的测试集为20%，它返回的是每组训练集与测试集的下标索引，由此可以知道哪些是train，那些是test。

**ylim**：tuple, shape (ymin, ymax), 可选的。定义绘制的最小和最大y值，这里是（0.7，1.01）。

**n_jobs** : 整数，可选并行运行的作业数（默认值为1）。windows开多线程需要在"__name__"==__main__中运行。

好了，以上为查阅资料以及文档对于上面参数的解释，下面看看运行的结果：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/65.png)

**绘制验证曲线**

在此图中，随着内核参数gamma的变化，显示了SVM的训练分数和验证分数。

对于非常低的gamma值，可以看到训练分数和验证分数都很低。这被称为欠配合。

gamma的中值是两个分数的高值，即分类器表现相当好。如果gamma太高，则分类

器将过度拟合，这意味着训练分数良好但验证分数较差。
```python
import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_digits from sklearn.svm import SVC from sklearn.model_selection import validation_curve digits = load_digits() X, y = digits.data, digits.target param_range = np.logspace(-6, -1, 5) train_scores, test_scores = validation_curve(    SVC(), X, y, param_name="gamma", param_range=param_range,    cv=10, scoring="accuracy", n_jobs=1) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.title("Validation Curve with SVM") plt.xlabel("$\gamma$") plt.ylabel("Score") plt.ylim(0.0, 1.1) lw = 2 #半对数坐标函数：只有一个坐标轴是对数坐标，另一个是普通算术坐标 plt.semilogx(param_range, train_scores_mean, label="Training score",             color="darkorange", lw=lw) #在区域内绘制函数包围的区域 plt.fill_between(param_range, train_scores_mean - train_scores_std,                 train_scores_mean + train_scores_std, alpha=0.2,                 color="darkorange", lw=lw) plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",             color="navy", lw=lw) plt.fill_between(param_range, test_scores_mean - test_scores_std,                 test_scores_mean + test_scores_std, alpha=0.2,                 color="navy", lw=lw) plt.legend(loc="best") plt.show()
```
![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/66.png)

https://blog.csdn.net/qq_36523839/article/details/82556932

https://blog.csdn.net/hao5335156/article/details/81268731/

## **三、嵌入式特征选择**

**Lasso回归**

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/67.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/68.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/69.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/70.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/71.png)

参考文献：

1.《机器学习》第十一章嵌入式选择与L1正则化——周志华

\2. [LASSO回归与L1正则化西瓜书](https://blog.csdn.net/BIT_666/article/details/80051737)

\3. [机器学习之正则化（正规化）](https://www.cnblogs.com/jianxinzhou/p/4083921.html)

\4. [正则化及正则化项的理解](https://blog.csdn.net/gshgsh1228/article/details/52199870/)

**Ridge回归**

岭回归不抛弃任何一个特征，缩小了回归系数。

岭回归求解与一般线性回归一致。

（1）如果采用梯度下降法：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\b8daeb9057764ebe9afeeab894a03f00\08.gif)

（6）

迭代公式如下：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\ce0da79ccf594a339e5e63582bcc8210\09.gif)

（7）

（2）如果采用正规方程：

最优解为：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\a4a2a37ce49b4cac9dcc37ae5bc7383d\10.gif)

（8）

最后，将学得的线性回归模型为：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\0545fd9e760e45d1872427c72f5e7a05\11.gif)

（9）

https://blog.csdn.net/pxhdky/article/details/82960659

**决策树**

https://www.cnblogs.com/xiemaycherry/p/10475067.html 

Descision Tree

**什么是决策树**

举个校园相亲的例子，今天校园的小猫(女)和小狗(男)准备配对，小猫如何才能在众多的优质🐶的心仪的狗呢？于是呢？有一只特乖巧的小猫找到了你，你正在学习机器学习，刚好学习了决策树，准备给这只猫猫挑选优质狗，当然，你不仅仅是直接告诉猫哪些狗是合适你的？你更应该详细的给猫讲解决策树是如何根据它提出的标准选出的符合要求的狗呢？

猫给出如下信息：

年龄<0.5 不心仪；年龄大于>=0.5 6.5<=体重<=8.5;心仪; 年龄>=0.5 体重>8.5 长相好 心仪;其余情况不心仪; 根据上述条件可以构造一颗树：

![img](C:\Users\14675\AppData\Local\YNote\data\software_hb@163.com\6c0b286a226341489c76ec2f8eea9409\72.png)

上面的图就是决策树，最终的结果是心仪或者不心仪。决策树算法以树形结构表示数据分类的结果

**基本概念**

决策树属于也只能非参数学习算法、可以用于解决(多)分类问题，回归问题。 回归问题的结果，叶子结点的平均值是回归问题的解。

根节点：决策树具有数据结构里面的二叉树、树的全部属性

非叶子节点 ：（决策点） 代表测试的条件，数据的属性的测试

叶子节点 ：分类后获得分类标记

分支： 测试的结果

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/73.png)

**决策树如何构建的问题**

自我提问阶段：

自我提问阶段：

每个节点的位置如何确定？

特征的选择：每次选入的特征作为分裂的标准，都是使得决策树在这个节点的根据你自己选择的标准（信息熵最小、信息增益最大、gini系数最小）.

每个节点在哪个值上做划分，确定分支结构呢？

遍历划分的节点的分界值操作来解决这个问题

可以想象，我们构造的决策树足够庞大，决策树可以把每一个样本都分对，那么决策树的泛化能力就可以很差了

为了解决这个问题，就需要剪枝操作了

**训练算法**

**基于信息熵的构造**

当选择某个特征作为节点时，我们就希望这个特征的信息熵越小越好，那么不确定性越小

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/74.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/75.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/76.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/77.png)

1.确定特征，统计属性值和分解结果，总共四个特征，四种特征的统计结果如下图：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/78.jepg)

\2. 根据历史数据，在不知到任何情况下，计算数据本身的熵为

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/79.png)

\3. 计算每个特征做为节点的信息熵

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/80.jpeg)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/81.png)

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/82.png)

**过拟合**

如果决策树过于庞大，分支太多，可能造成过拟合。对应训练样本都尽可能的分对，也许样本本身就存在异常点呢？

1. 指定深度d
2. 节点的min_sample
3. 节点熵值或者gini值小于阙值熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。
4. 当所以特征都用完了
5. 指定节点个数

当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。

II. 后剪枝： 构建好后，然后才开始裁剪

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/83.png)

在构造含一棵树后，选一些节点做计算，看是否需要剪枝

**随机森林**

Bootstraping: 有放回采样

Bagging:有放回采样n个样本，建立分类器

一次采样一颗树，多次采样多棵树，构成一片森林，多个分类器共同决定。当有一个test时，代入所以的决策树，共同决策。

随机性的解释：

\1. 数据的随机性

为每个树选取训练数据，随机按照比列有放回选择数据集

\2. 特征的随机性

按照比列选取特征的个数

**决策树单个节点选择的代码实现**

简单实现了单个节点决策构造过程
```python
def split(X,y,d,value): ''' 在d纬度上，按照value进行划分 '''    index_a =(X[:,d]<=value)    index_b =(X[:,d]>value)    return X[index_a],X[index_b],y[index_a],y[index_b] from collections import Counter from math import log  from numpy as np def entropy(y):    counter = Counter(y) # 字典    res = 0.0    for num in counter.values():        p = num/len(y)        res+=-p*log(p)    return res def gain(X,y,d,v):    X_l,X_r,y_l,y_r = split(X,y,d,v)    e = len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)    return (entropy(y)-e) def gainratio(X,y,d,v):    X_l,X_r,y_l,y_r = split(X,y,d,v)    gain =entropy(y) - len(y_l)/len(y)*entropy(y_l)+len(y_r)/len(y)*entropy(y_r)    return gain/(entropy(y_l)+entropy(y_r)) def gini(y):    counter = Counter(y)    res = 1.0    for num in counter.values():        p = num / len(y)        res += -p**2    return res    #X_l,X_r,y_l,y_r = split(X,y,d,v)    #return 1-(len(y_l)/len(y))**2-(len(y_r)/len(y))**2 def try_split(X,y):    best_entropy = float('inf')    best_d,best_v=-1,-1    for d in range(X.shape[1]):        sorted_index = np.argsort(X[:,d])                for i in range(1, len(X)):            if (X[sorted_index[i],d] != X[sorted_index[i-1],d]):                v = (X[sorted_index[i-1],d]+X[sorted_index[i],d])/2                X_l,X_r,y_l,y_r = split(X,y,d,v)                # 信息熵                e = entropy(y_l)+entropy(y_r)                #gini                e = gini(y_l) + gini(y_r)                # 信息增益                e = -gain(X,y,d,v)                 if e < best_entropy:                    best_entropy, best_d,best_v = e,d,v    return best_entropy, best_d, best_v # 手动来划分 data =np.array([[	0.3	,	5	,	2	,	0	], [	0.4	,	6	,	0	,	0	], [	0.5	,	6.5	,	1	,	1	], [	0.6	,	6	,	0	,	0	], [	0.7	,	9	,	2	,	1	], [	0.5	,	7	,	1	,	0	], [	0.4	,	6	,	0	,	0	], [	0.6	,	8.5	,	0	,	1	], [	0.3	,	5.5	,	2	,	0	], [	0.9	,	10	,	0	,	1	], [	1	,	12	,	1	,	0	], [	0.6	,	9	,	1	,	0	], ]) X =data[:,0:3] y = data[:,-1] # 手动来划分 best_entropy, best_d, best_v = try_split(X, y) print(best_entropy, best_d, best_v) X1_l, X1_r, y1_l, y1_r = split(X,y,best_d,best_v) print(X1_l, X1_r, y1_l, y1_r)  best_entropy2, best_d2, best_v2 = try_split(X1_r, y1_r) X2_l, X2_r, y2_l, y2_r = split(X1_r,y1_r,best_d2,best_v2) entropy(y2_l)
```
**Python sklean里面tree模块里面的DecisionTreeClassifier**
```python
from sklearn import tree clf =tree.DecisionTreeClassifier(max_depth=1,criterion ='gini') # criterion='entropy|gini' clf = clf.fit(X,y)

训练好一颗决策树之后，我们可以使用export_graphviz导出器以Graphviz格式导出树。

import graphviz  dot_data = tree.export_graphviz(clf, out_file=None,)  graph = graphviz.Source(dot_data)  graph.render("data") 
```
## **四、模型对比**

**常用线性回归模型**

1 原理

1.1 引入

线性回归是最为常用的一种数据分析手段，通常我们拿到一组数据后，都会先看一看数据中各特征之间是否存在明显的线性关系。例如，现在我们拿到了一组学校中所有学生基本资料的数据，该数据以二维表格的形式呈现，如下表所示。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/84.jpeg)

示例数据表

每行代表一个学生，每列代表该学生的一个属性（或称为特征），那么如果我们对特征进行仔细观察，不难发现身高和年龄总是呈现正相关关系，数学成绩与物理成绩也基本呈现正相关关系。那么我们是否可以给这样的两个特征之间拟合出一条近似的直线来表达他们之间的线性函数关系呢？这里我们的想法其实就是机器学习的世界观：数据驱动构建模型。

1.2 模型

只不过这里的模型非常简单，只是线性模型，也就是一条直线方程，通长我们可以表达成如下公式：

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D+%3D+wx%2Bb)

这里， 

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)

 是代表了预测值，数据中我们将某一特征列作为自变量 

![img](https://www.zhihu.com/equation?tex=x)

 (例如身高)，因变量 

![img](https://www.zhihu.com/equation?tex=y)

 (如体重)也就是我们想要预测的值， 

![img](https://www.zhihu.com/equation?tex=x)

 和 

![img](https://www.zhihu.com/equation?tex=y)

 都已知，现在的任务就是，加入新增了一个 

![img](https://www.zhihu.com/equation?tex=x)

 ，而其对应的 

![img](https://www.zhihu.com/equation?tex=y)

 未知，那么我们该如何预测出一个 

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)

 ？显然，我们需要构建 

![img](https://www.zhihu.com/equation?tex=y)

 与 

![img](https://www.zhihu.com/equation?tex=x)

 之间的函数关系:

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D+%3Df%28x%29)

 ，对于身高体重这样的简单问题而言，就可以直接使用上述的线性方程作为我们想要拟合的模型。

接下来的问题就是，如何拟合这个模型，也就是说，如何求得线性模型中的两个参数 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 ？

1.3 损失函数

要求解最佳的参数，首先我们需要让计算机知道一个目标，毕竟解决任何问题都需要确立一个明确的目标才行，对于计算机这样的数字世界，我们就需要给它确定一个定量化的目标函数式，在优化问题中，我们通常称之为目标函数，或者损失函数(Loss function)。无论我们选择什么样的模型，最终都是可以得到一组预测值 

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D)

 ，对比已有的真实值 

![img](https://www.zhihu.com/equation?tex=y)

 ，数据行数为 

![img](https://www.zhihu.com/equation?tex=n)

 ，我们很自然地可以将损失函数定义如下：

![img](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28%5Chat%7By%7D_i+-+y_i%29%5E2)

即预测值与真实值之间的平均的平方距离，统计中我们一般称其为MAE(mean square error)均方误差。把之前我们确定的 

![img](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%3Dwx%2Bb)

 带入损失函数：

![img](https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28f%28x_i%29+-+y_i%29%5E2+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28wx_i%2Bb+-+y_i%29%5E2)

注意，对于损失函数 

![img](https://www.zhihu.com/equation?tex=L)

 而言，其自变量不再是我们习惯中的 

![img](https://www.zhihu.com/equation?tex=x)

 （其实 

![img](https://www.zhihu.com/equation?tex=x)

 和 

![img](https://www.zhihu.com/equation?tex=y)

 都是在训练数据中的已知值），损失函数 

![img](https://www.zhihu.com/equation?tex=L)

 的自变量应该是我们要求解的参数 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

，因此我们可以把损失函数重新记为：

![img](https://www.zhihu.com/equation?tex=L%28w%2Cb%29%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28wx_i%2Bb+-+y_i%29%5E2)

现在，我们的任务就是希望把这个损失函数交给计算机，然后跟计算机说，帮我把这个函数最小化，然后告诉我 

![img](https://www.zhihu.com/equation?tex=L)

 最小时的一组 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 是多少就行了。但是显然计算机还没那么聪明，它并不知道怎么算，我们还是要靠自己解决。

核心的优化目标式：

![img](https://www.zhihu.com/equation?tex=%28w%5E%2A%2C+b%5E%2A%29+%3D+%5Carg+%5Cmin_%7B%28w%2Cb%29%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28wx_i%2Bb+-+y_i%29%5E2)

这里有两种方式：

- 一种是“最小二乘法”(least square method)，可直接求解；
- 另一种是梯度下降(gradient descent)，有关梯度下降的方法原理可参考我之前这篇文章 -> [[link\]](https://zhuanlan.zhihu.com/p/36564434).

1.4 最小二乘法

求解 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 是使损失函数最小化的过程，在统计中，称为线性回归模型的最小二乘“参数估计”(parameter estimation)。我们可以将 

![img](https://www.zhihu.com/equation?tex=L%28w%2Cb%29)

 分别对 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 求导，得到：

![img](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+w%7D+%3D+2+%5Cleft%28+w%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx%5E2+-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i%28y_i+-+b%29+%5Cright%29)

![img](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+b%7D+%3D+2+%5Cleft%28+nb+-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28y_i+-+wx_i%29+%5Cright%29)

令上述两式为0，可得到 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 最优解的闭式(closed-form)解：

![img](https://www.zhihu.com/equation?tex=w+%3D+%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5En+y_i+%28x_i+-+%5Cbar%7Bx%7D%29%7D%7B%5Csum_%7Bi%3D1%7D%5En+x_i%5E2+-+%5Cfrac%7B1%7D%7Bn%7D%5Cleft%28+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dx_i+%5Cright%29%5E2%7D)

![img](https://www.zhihu.com/equation?tex=b+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28y_i+-+wx_i%29)

其中， 

![img](https://www.zhihu.com/equation?tex=%5Cbar%7Bx%7D+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+x_i)

 为 

![img](https://www.zhihu.com/equation?tex=x)

 的均值。

1.5 梯度下降法求解

参考之前[关于梯度下降的文章](https://zhuanlan.zhihu.com/p/36564434)，根据其中的自变量更新公式 

![img](https://www.zhihu.com/equation?tex=x%27+%5Cleftarrow+x+-+%5Calpha+%5Ctriangledown+f%28x%29)

 ，应用于当前的线性模型，可得 

![img](https://www.zhihu.com/equation?tex=w)

 和 

![img](https://www.zhihu.com/equation?tex=b)

 的更新公式为：

![img](https://www.zhihu.com/equation?tex=w+%5Cleftarrow+w+-+%5Calpha+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+w%7D)

![img](https://www.zhihu.com/equation?tex=b+%5Cleftarrow+b+-+%5Calpha+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+b%7D)

故：

![img](https://www.zhihu.com/equation?tex=w+%5Cleftarrow+w+-+%5Calpha+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28wx_i+%2B+b+-+y_i%29+x_i)

![img](https://www.zhihu.com/equation?tex=b+%5Cleftarrow+b+-+%5Calpha+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%28wx_i+%2B+b+-+y_i%29)

其中， 

![img](https://www.zhihu.com/equation?tex=%5Calpha)

 一个大于0的正数，通常称之为步长或学习率(learning rate)。

2 代码实现 (使用梯度下降法)

完整代码可参考：[zlxy9892/ml_code](https://link.zhihu.com/?target=https%3A//github.com/zlxy9892/ml_code/tree/master/basic_algorithm/liner_regression)

首先建立 [liner_regression.py](https://link.zhihu.com/?target=https%3A//github.com/zlxy9892/ml_code/blob/master/basic_algorithm/liner_regression/liner_regression.py) 文件，用于实现线性回归的类文件，包含了线性回归内部的核心函数：
```python
*# -\*- coding: utf-8 -\*-* import numpy as np  class LinerRegression(object):     def __init__(self, learning_rate=0.01, max_iter=100, seed=None):        np.random.seed(seed)        self.lr = learning_rate        self.max_iter = max_iter        self.w = np.random.normal(1, 0.1)        self.b = np.random.normal(1, 0.1)        self.loss_arr = []     def fit(self, x, y):        self.x = x        self.y = y        for i in range(self.max_iter):            self._train_step()            self.loss_arr.append(self.loss())            *# print('loss: \t{:.3}'.format(self.loss()))*            *# print('w: \t{:.3}'.format(self.w))*            *# print('b: \t{:.3}'.format(self.b))*     def _f(self, x, w, b):        return x * w + b     def predict(self, x=None):        if x is None:            x = self.x        y_pred = self._f(x, self.w, self.b)        return y_pred     def loss(self, y_true=None, y_pred=None):        if y_true is None or y_pred is None:            y_true = self.y            y_pred = self.predict(self.x)        return np.mean((y_true - y_pred)**2)     def _calc_gradient(self):        d_w = np.mean((self.x * self.w + self.b - self.y) * self.x)        d_b = np.mean(self.x * self.w + self.b - self.y)        return d_w, d_b     def _train_step(self):        d_w, d_b = self._calc_gradient()        self.w = self.w - self.lr * d_w        self.b = self.b - self.lr * d_b        return self.w, self.b
```
建立 [train.py](https://link.zhihu.com/?target=https%3A//github.com/zlxy9892/ml_code/blob/master/basic_algorithm/liner_regression/train.py) 文件，用于生成模拟数据，并调用 liner_regression.py 中的类，完成线性回归任务：
```python
*# -\*- coding: utf-8 -\*-* import numpy as np import matplotlib.pyplot as plt from liner_regression import *  def show_data(x, y, w=None, b=None):    plt.scatter(x, y, marker='.')    if w is not None and b is not None:        plt.plot(x, w*x+b, c='red')    plt.show()  *# data generation* np.random.seed(272) data_size = 100 x = np.random.uniform(low=1.0, high=10.0, size=data_size) y = x * 20 + 10 + np.random.normal(loc=0.0, scale=10.0, size=data_size) *# plt.scatter(x, y, marker='.')* *# plt.show()* *# train / test split* shuffled_index = np.random.permutation(data_size) x = x[shuffled_index] y = y[shuffled_index] split_index = int(data_size * 0.7) x_train = x[:split_index] y_train = y[:split_index] x_test = x[split_index:] y_test = y[split_index:] *# visualize data* *# plt.scatter(x_train, y_train, marker='.')* *# plt.show()* *# plt.scatter(x_test, y_test, marker='.')* *# plt.show()* *# train the liner regression model* regr = LinerRegression(learning_rate=0.01, max_iter=10, seed=314) regr.fit(x_train, y_train) print('cost: \t{:.3}'.format(regr.loss())) print('w: \t{:.3}'.format(regr.w)) print('b: \t{:.3}'.format(regr.b)) show_data(x, y, regr.w, regr.b) *# plot the evolution of cost* plt.scatter(np.arange(len(regr.loss_arr)), regr.loss_arr, marker='o', c='green') plt.show()
```
显示结果：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/85.jpeg)

原始数据x, y和拟合的直线方程

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/86.jpeg)

利用梯度下降法优化过程中损失函数的下降情况

https://zhuanlan.zhihu.com/p/36616740

**常用非线性模型**

在目前的机器学习领域中，最常见的三种任务就是：回归分析、分类分析、聚类分析。那么什么是回归呢？回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析在机器学习领域应用非常广泛，例如，商品的销量预测问题，交通流量预测问题。下面介绍几种常见的非线性回归模型。

1、SVR

众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。
```python
\#加载SVR模型算法库 from sklearn.svm import SVR #训练集 y = [0.5, 2.5] #创建SVR回归模型的对象 clf = SVR() # 利用训练集训练SVR回归模型 clf.fit(X, y)  """ SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,    tol=0.001, verbose=False) """ clf.predict([[1, 1]])
```
2、决策树回归

from sklearn.tree import  DecisionTreeRegressor  X = [[0, 0], [2, 2]] y = [0.5, 2.5] clf = DecisionTreeRegressor() clf = clf.fit(X, y) clf.predict([[1, 1]])

3、knn回归

在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。
```python
X = [[0], [1], [2], [3]] y = [0, 0, 1, 1] from sklearn.neighbors import KNeighborsRegressor neigh = KNeighborsRegressor(n_neighbors=2) neigh.fit(X, y)  print(neigh.predict([[1.5]]))
```
4、RandomForest回归

随机森林回归算法也是一种经典的集成算法之一。
```pyhton
from sklearn.ensemble import RandomForestRegressor from sklearn.datasets import make_regression X, y = make_regression(n_features=4, n_informative=2,                       random_state=0, shuffle=False) regr = RandomForestRegressor(max_depth=2, random_state=0,                             n_estimators=100) regr.fit(X, y) print(regr.feature_importances_) print(regr.predict([[0, 0, 0, 0]]))
```
5、XGBoost回归

XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法
```python
import xgboost as xgb xgb_model = xgb.XGBRegressor(max_depth = 3,                             learning_rate = 0.1,                             n_estimators = 100,                             objective = 'reg:linear',                             n_jobs = -1) xgb_model.fit(X_train, y_train,              eval_set=[(X_train, y_train)],               eval_metric='logloss',              verbose=100) y_pred = xgb_model.predict(X_test) print(mean_squared_error(y_test, y_pred))
```
6、神经网络MLP回归

神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。
```python
from sklearn.neural_network import MLPRegressor mlp=MLPRegressor() mlp.fit(X_train,y_train) """ MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,       beta_2=0.999, early_stopping=False, epsilon=1e-08,       hidden_layer_sizes=(100,), learning_rate='constant',       learning_rate_init=0.001, max_iter=200, momentum=0.9,       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,       random_state=None, shuffle=True, solver='adam', tol=0.0001,       validation_fraction=0.1, verbose=False, warm_start=False) """ y_pred = mlp.predict(X_test)

y_pred = mlp.predict(X_test)
```
7、LightGBM回归

LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。
```python
import lightgbm as lgb gbm = lgb.LGBMRegressor(num_leaves=31,                        learning_rate=0.05,                        n_estimators=20) gbm.fit(X_train, y_train,        eval_set=[(X_train, y_train)],         eval_metric='logloss',        verbose=100) y_pred = gbm.predict(X_test) print(mean_squared_error(y_test, y_pred))
```
8、GBDT回归

集成算法Boosting的一种，基于决策树的梯度提升树回归算法
```python
\# GBDT超参数调优 params = { 'n_estimators': 400,  'max_depth': 11, 'learning_rate': 0.06, 'loss': 'ls', 'subsample':0.8 } gbdt=GradientBoostingRegressor(**params) # 模型训练 gbdt.fit(X_train, y_train)
```
**二者差别：**

作者：王多鱼

链接：https://zhuanlan.zhihu.com/p/37866896

来源：知乎

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

**线性模型：**

在统计意义上，如果一个回归等式是线性的，那么它的相对于参数就必须也是线性的。如果相对于参数是线性，那么即使性对于样本变量的特征是二次方或者多次方，这个回归模型也是线性的 (例如下面的公式)。

![img](https://www.zhihu.com/equation?tex=Y%3D%5Comega_0%2B%5Comega_1x_1%2B%5Comega_2x_2%5E2)

甚至可以使用 log 或者指数去形式化特征：

![img](https://www.zhihu.com/equation?tex=Y%3D%5Comega_0%2B%5Comega_1exp%28-x_1%29%2B%5Comega_2exp%28-x_2%5E2%29)

**非线性模型：**

最简单的判断一个模型是不是非线性，就是关注非线性本身，判断它的参数是不是非线性的。非线性有很多种形象，这也是为什么非线性模型能够那么好的拟合那些曲折的函数曲线的原因。比如下面这个：

![img](https://www.zhihu.com/equation?tex=Y%3D%5Ctheta_1%5Ctimes+x%5E%7B%5Ctheta_2%7D)

![img](https://www.zhihu.com/equation?tex=Y%3D%5Ctheta_1%2B%28%5Ctheta_2-%5Ctheta_3%29%5Ctimes+exp%28-%5Ctheta_4x%29)

与线性模型不一样的是，这些非线性模型的特征因子对应的参数不止一个。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/87.png)

机器学习中线性模型和非线性的区别 - 王多鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/37866896

## **五、模型调参**

**贪心调参方法**

我主要调整的XGBoost参数如下：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/88.png)

​    利用网格搜索的方式来同时调优上面的参数是一个巨大计算量的工作，这限制了每个参数可取值空间的大小。所以我们采用贪婪的方式对参数进行分组后分步调优，并且每次并不是只依赖于最优的那个参数子集，而是会选取若干个最优的参数子集（所以我称算法为“适度贪婪”）。主要操作细节如下：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/89.png)

参数调整的取值范围：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/90.png)

​    基于贪心算法思想，我将XGBoost的调参过程分为六个步骤，在每一步调参后取得的局部最优参数条件下，再进行下一步的对其他参数调优；以此类推，直至调整完所有参数。

​    说明：对于reg_alpha，reg_lambda的两组参数调参范围如下，但是考虑到这两组参数是正则化的参数，并且如果再次进行8次调参的话太耗时，我没有再次对它进行8次调参，在目前得到的最优一组参数下（max_depth=3，min_child_weight=3，gamma=0.2，subsample=0.9，colsample_bytree=0.8）调整参数。详细数据如下：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/91.png)

**网格调参方法**

网格搜索，稍微比试错法效率好一点。因为方法简单，**适用于超参数种类比较少，取值也不多的情况下**，也会被很多工程师用到。

基本思路是这样的，比如现在要寻找一组Dropout rate和Learning rate的最佳组合，让验证集误差最小。网格搜索就是指定一些离散的Dropout rate，比如(0.4,0.5,0.6,0.7)这四个取值；然后再给Learning rate也同样指定一些可能的取值，比如(1e-4,1e-3,1e-2)这三个取值，如果以Dropout rate为横轴，以Learning rate为纵轴建立一个平面直角坐标系，那么这些取值点就是盘面上的4x3=12个点，这些点连成一个网格。下一步就是以这12个点为参数训练12个模型，取效果最好的参数。

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/92.jpeg)

这个方法的优点在于容易理解，概念简单，易于多机并行化，缺点在于穷举方法太暴力，**成本太高**，特别是需要调节的参数数量比较多的时候，训练次数是**成指数性增长**的。

**贝叶斯调参方法**

相比之下，试错法和二分搜索，都会根据上一次的训练结果来做分析，根据分析结果指导下一次训练超参数的设置。

贝叶斯超参数优化方法则是有别于照看法和二分法的另一种更精巧的**可以根据前面的训练结果，来指导后面训练超参数设置**的超参数优化方法。

超参数搜索其实可以转化为一个优化问题，L = f(x)，其中x是超参数的取值，f是从这组超参数到训练出来的模型在验证集误差的映射。在及其简化的条件下，可以计算验证集上可导误差函数关于超参数的梯度来求最佳超参数，但更多的时候这个导数是找不到的。贝叶斯优化不求找到这个导数，它使用一个代理模型（例如高斯过程），这个代理模型**不仅将预测转化为一个值，还为我们提供在其它地方不确定性的范围（均值和方差）**，然后下一次选择的超参数就在**不确定性最大的**地方选择，因为选择不确定性最大的地方的超参数，可以提供**最大量的信息**。

下面以一个超参数的贝叶斯优化来举例：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/93.jpeg)

上图中红色点线是验证集误差的真值，黑色曲线是我们对真值预测的平均值，灰色代表预测的不确定性，黑点是我们已经跑过的实验，不确定性为0，肯定和误差的真值一样，但离黑点远的地方，我们拥有的信息少，不确定性大。而底下蓝色的是不确定性，蓝色的点是指示下一次超参数应该选择的地方（就是不确定性最大的地方）。下图是根据上图的指示的点进行的一次迭代：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/94.jpeg)

迭代之后，可以看到，多了一个黑点，我们知道了更多的关于f的信息，并更新了不确定性的分布。再在下一次挑选超参数的时候，依然选择不确定性最大的地方的超参数值来训练，如此往复，直到预先设定的标准满足或者资源耗尽。

下图是如此往复实验8个超参数的结果：

![img](https://github.com/makeittrue/data-mining_learning_note/blob/master/Task04/images/95.jpeg)

贝叶斯方法关键涉及到一个代理模型的选择，例如上图中的EI(x)的定义，现在**还没有很成熟可靠**的方法。有时候贝叶斯方法表现的像人类专家，在某些问题上可以取得比较好的结果，但有时候在某些问题上又像白痴一样。