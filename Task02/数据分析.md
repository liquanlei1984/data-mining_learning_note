# EDA-数据探索性分析

## EDA目标
- EDA的价值主要在于熟悉数据集，了解数据集，对数据集进行验证来确定所获得数据集可以用于接下来的机器学习或者深度学习使用。

- 当了解了数据集之后我们下一步就是要去了解变量间的相互关系以及变量与预测值之间的存在关系。

- 引导数据科学从业者进行数据处理以及特征工程的步骤,使数据集的结构和特征集让接下来的预测问题更加可靠。

- 完成对于数据的探索性分析，并对于数据进行一些图表或者文字总结并打卡。

## 内容介绍
1.  载入各种数据科学以及可视化库:
    - 数据科学库 pandas、numpy、scipy；
    - 可视化库 matplotlib、seabon；
    - 其他；
2.  载入数据：
    - 载入训练集和测试集；
    - 简略观察数据(head()+shape)；
3.  数据总览:
    - 通过describe()来熟悉数据的相关统计量
    - 通过info()来熟悉数据类型
4.  判断数据缺失和异常
    - 查看每列的存在nan情况
    - 异常值检测
5.  了解预测值的分布
    - 总体分布概况（无界约翰逊分布等）
    - 查看skewness and kurtosis
    - 查看预测值的具体频数
6.  特征分为类别特征和数字特征，并对类别特征查看unique分布
7.  数字特征分析
    - 相关性分析
    - 查看几个特征得 偏度和峰值
    - 每个数字特征得分布可视化
    - 数字特征相互之间的关系可视化
    - 多变量互相回归关系可视化
8.  类型特征分析
    - unique分布
    - 类别特征箱形图可视化
    - 类别特征的小提琴图可视化
    - 类别特征的柱形图可视化类别
    - 特征的每个类别频数可视化(count_plot)
9. 用pandas_profiling生成数据报告

## 代码示例
### 载入各种数据科学以及可视化库
In[1]:
```python
    #coding:utf-8
    #导入warnings包，利用过滤器来实现忽略警告语句。
    import warnings
    warnings.filterwarnings('ignore')

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    import missingno as msno
```
### 载入数据
In[2]:
```python
    ## 1) 载入训练集和测试集；
    Train_data = pd.read_csv('train.csv', sep=' ')
    Test_data = pd.read_csv('testA.csv', sep=' ')
```


- name - 汽车编码
- regDate - 汽车注册时间
- model - 车型编码
- brand - 品牌
- bodyType - 车身类型 
- fuelType - 燃油类型
- gearbox - 变速箱
- power - 汽车功率
- kilometer - 汽车行驶公里
- notRepairedDamage - 汽车有尚未修复的损坏
- regionCode - 看车地区编码
- seller - 销售方
- offerType - 报价类型
- creatDate - 广告发布时间
- price - 汽车价格
- v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14' 【匿名特征，包含v0-14在内15个匿名特征】

In[3]:
```python
    ## 2) 简略观察数据(head()+shape)
    Train_data.head().append(Train_data.tail())
```


In[4]:
```python
    Train_data.shape
```
   

In[5]:
```python
    Test_data.head().append(Test_data.tail())
```

In[6]:
```python
    Test_data.shape
```   

要养成看数据集的head()以及shape的习惯，这会让你每一步更放心，导致接下里的连串的错误, 如果对自己的pandas等操作不放心，建议执行一步看一下，这样会有效的方便你进行理解函数并进行操作.

## 总览数据概况
1. describe种有每列的统计量，个数count、平均值mean、方差std、最小值min、中位数25% 50% 75% 、以及最大值 看这个信息主要是瞬间掌握数据的大概的范围以及每个值的异常值的判断，比如有的时候会发现999 9999 -1 等值这些其实都是nan的另外一种表达方式，有的时候需要注意下。
2. info 通过info来了解数据每列的type，有助于了解是否存在除了nan以外的特殊符号异常。

In[7]:
```python
    ## 1) 通过describe()来熟悉数据的相关统计量
    Train_data.describe()

```

In[8]:
```python
    ## 2) 通过info()来熟悉数据类型
    Train_data.info()
```

In[9]:
```python
    Test_data.info()
```


## 判断数据缺失和异常
In[10]:
```python
    ## 1) 查看每列的存在nan情况
    Train_data.isnull().sum()
```

In[11]:
```python
    Test_data.isnull().sum()
```

In[12]:
```python
    # nan可视化
    missing = Train_data.isnull().sum()
    missing = missing[missing > 0]
    missing.sort_values(inplace=True)
    missing.plot.bar()
```


通过以上两句可以很直观的了解哪些列存在 “nan”, 并可以把nan的个数打印，主要的目的在于 nan存在的个数是否真的很大，如果很小一般选择填充，如果使用lgb等树模型可以直接空缺，让树自己去优化，但如果nan存在的过多、可以考虑删掉。

In[13]:
```python
    # 可视化看下缺省值
    msno.matrix(Train_data.sample(250))
```


In[14]:
```python
    msno.bar(Train_data.sample(1000))
```


In[15]:
```python
    # 可视化看下缺省值
    msno.matrix(Test_data.sample(250))
```

In[16]:
```python
    msno.bar(Test_data.sample(1000))
```

In[17]:
```python
    ## 2) 查看异常值检测
    Train_data.info()
```


可以发现除了notRepairedDamage 为object类型其他都为数字 这里我们把他的几个不同的值都进行显示就知道了。

In[18]:
```python
    Train_data['notRepairedDamage'].value_counts()
```

In[19]:
```python
    Train_data['notRepairedDamage'].replace('-', np.nan, inplace=True)
    Train_data['notRepairedDamage'].value_counts()
```

In[20]:
```python
    Train_data.isnull().sum()
```

In[21]:
```python
    Test_data['notRepairedDamage'].value_counts()
```

In[22]:
```python
    Test_data['notRepairedDamage'].replace('-', np.nan, inplace=True)
    Train_data["seller"].value_counts()

```

In[23]:
```python
    Train_data["offerType"].value_counts()
```

In[24]:
```python
    del Train_data["seller"]
    del Train_data["offerType"]
    del Test_data["seller"]
    del Test_data["offerType"]
```

In[25]:
```python
    Train_data['price']
```

In[26]:
```python
    Train_data['price'].value_counts()
```

In[27]:
```python
    ## 1) 总体分布概况（无界约翰逊分布等）
    import scipy.stats as st
    y = Train_data['price']
    plt.figure(1); plt.title('Johnson SU')
    sns.distplot(y, kde=False, fit=st.johnsonsu)
    plt.figure(2); plt.title('Normal')
    sns.distplot(y, kde=False, fit=st.norm)
    plt.figure(3); plt.title('Log Normal')
    sns.distplot(y, kde=False, fit=st.lognorm)
```

价格不服从正态分布，所以在进行回归之前，它必须进行转换。虽然对数变换做得很好，但最佳拟合是无界约翰逊分布

In[28]:
```python
    ## 2) 查看skewness and kurtosis
    sns.distplot(Train_data['price']);
    print("Skewness: %f" % Train_data['price'].skew())
    print("Kurtosis: %f" % Train_data['price'].kurt())
```

In[29]:
```python
    Train_data.skew(), Train_data.kurt()
```

In[30]:
```python
    sns.distplot(Train_data.skew(),color='blue',axlabel ='Skewness')
```

In[31]:
```python
    sns.distplot(Train_data.kurt(),color='orange',axlabel ='Kurtness')

```

skew、kurt说明参考:https://www.cnblogs.com/wyy1480/p/10474046.html

In[32]:
```python
    ## 3) 查看预测值的具体频数
    plt.hist(Train_data['price'], orientation = 'vertical',histtype = 'bar', color ='red')
    plt.show()
```


查看频数, 大于20000得值极少，其实这里也可以把这些当作特殊得值（异常值）直接用填充或者删掉，再前面进行

In[33]:
```python
    # log变换 z之后的分布较均匀，可以进行log变换进行预测，这也是预测问题常用的trick
    plt.hist(np.log(Train_data['price']), orientation = 'vertical',histtype = 'bar', color ='red') 
    plt.show()

```

## 特征分为类别特征和数字特征，并对类别特征查看unique分布

**数据类型**

**列**
* name - 汽车编码
* regDate - 汽车注册时间
* model - 车型编码
* brand - 品牌
* bodyType - 车身类型
* fuelType - 燃油类型
* gearbox - 变速箱
* power - 汽车功率
* kilometer - 汽车行驶公里
* notRepairedDamage - 汽车有尚未修复的损坏
* regionCode - 看车地区编码
* seller - 销售方 【以删】
* offerType - 报价类型 【以删】
* creatDate - 广告发布时间
* price - 汽车价格
* v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14'（根据汽车的评论、标签等大量信息得到的embedding向量）【人工构造 匿名特征】

In[34]:
```python
    # 分离label即预测值
    Y_train = Train_data['price']

    # 这个区别方式适用于没有直接label coding的数据
    # 这里不适用，需要人为根据实际含义来区分
    # 数字特征
    # numeric_features = Train_data.select_dtypes(include=[np.number])
    # numeric_features.columns
    # # 类型特征
    # categorical_features = Train_data.select_dtypes(include=[np.object])
    # categorical_features.columns

    numeric_features = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14' ]

    categorical_features = ['name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'regionCode',]

    # 特征nunique分布
    for cat_fea in categorical_features:
        print(cat_fea + "的特征分布如下：")
        print("{}特征有个{}不同的值".format(cat_fea, Train_data[cat_fea].nunique()))
        print(Train_data[cat_fea].value_counts())
```


In[35]:
```python
    # 特征nunique分布
    for cat_fea in categorical_features:
        print(cat_fea + "的特征分布如下：")
        print("{}特征有个{}不同的值".format(cat_fea, Test_data[cat_fea].nunique()))
        print(Test_data[cat_fea].value_counts())
```

## 数字特征分析

In[36]:
```python
    numeric_features.append('price')
    numeric_features
```

In[37]:
```python
    Train_data.head()
```

In[38]:
```python
    ## 1) 相关性分析
    price_numeric = Train_data[numeric_features]
    correlation = price_numeric.corr()
    print(correlation['price'].sort_values(ascending = False),'\n')
```

In[39]:
```python
    f , ax = plt.subplots(figsize = (7, 7))

    plt.title('Correlation of Numeric Features with Price',y=1,size=16)

    sns.heatmap(correlation,square = True,  vmax=0.8)
```

In[40]:
```python
    del price_numeric['price']

    ## 2) 查看几个特征得 偏度和峰值
    for col in numeric_features:
        print('{:15}'.format(col), 
            'Skewness: {:05.2f}'.format(Train_data[col].skew()) , 
            '   ' ,
            'Kurtosis: {:06.2f}'.format(Train_data[col].kurt())  
            )

```

In[41]:
```python
    ## 3) 每个数字特征得分布可视化
    f = pd.melt(Train_data, value_vars=numeric_features)
    g = sns.FacetGrid(f, col="variable",  col_wrap=2, sharex=False, sharey=False)
    g = g.map(sns.distplot, "value")
```

In[42]:
```python
    ## 4) 数字特征相互之间的关系可视化
    sns.set()
    columns = ['price', 'v_12', 'v_8' , 'v_0', 'power', 'v_5',  'v_2', 'v_6', 'v_1', 'v_14']
    sns.pairplot(Train_data[columns],size = 2 ,kind ='scatter',diag_kind='kde')
    plt.show()
```

In[43]:
```python
    Train_data.columns
```

In[44]:
```python
    Y_train
```


此处是多变量之间的关系可视化，可视化更多学习可参考很不错的文章 https://www.jianshu.com/p/6e18d21a4cad

In[45]:
```python
## 5) 多变量互相回归关系可视化
    fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(nrows=5, ncols=2, figsize=(24, 20))
    # ['v_12', 'v_8' , 'v_0', 'power', 'v_5',  'v_2', 'v_6', 'v_1', 'v_14']
    v_12_scatter_plot = pd.concat([Y_train,Train_data['v_12']],axis = 1)
    sns.regplot(x='v_12',y = 'price', data = v_12_scatter_plot,scatter= True, fit_reg=True, ax=ax1)

    v_8_scatter_plot = pd.concat([Y_train,Train_data['v_8']],axis = 1)
    sns.regplot(x='v_8',y = 'price',data = v_8_scatter_plot,scatter= True, fit_reg=True, ax=ax2)

    v_0_scatter_plot = pd.concat([Y_train,Train_data['v_0']],axis = 1)
    sns.regplot(x='v_0',y = 'price',data = v_0_scatter_plot,scatter= True, fit_reg=True, ax=ax3)

    power_scatter_plot = pd.concat([Y_train,Train_data['power']],axis = 1)
    sns.regplot(x='power',y = 'price',data = power_scatter_plot,scatter= True, fit_reg=True, ax=ax4)

    v_5_scatter_plot = pd.concat([Y_train,Train_data['v_5']],axis = 1)
    sns.regplot(x='v_5',y = 'price',data = v_5_scatter_plot,scatter= True, fit_reg=True, ax=ax5)

    v_2_scatter_plot = pd.concat([Y_train,Train_data['v_2']],axis = 1)
    sns.regplot(x='v_2',y = 'price',data = v_2_scatter_plot,scatter= True, fit_reg=True, ax=ax6)

    v_6_scatter_plot = pd.concat([Y_train,Train_data['v_6']],axis = 1)
    sns.regplot(x='v_6',y = 'price',data = v_6_scatter_plot,scatter= True, fit_reg=True, ax=ax7)

    v_1_scatter_plot = pd.concat([Y_train,Train_data['v_1']],axis = 1)
    sns.regplot(x='v_1',y = 'price',data = v_1_scatter_plot,scatter= True, fit_reg=True, ax=ax8)

    v_14_scatter_plot = pd.concat([Y_train,Train_data['v_14']],axis = 1)
    sns.regplot(x='v_14',y = 'price',data = v_14_scatter_plot,scatter= True, fit_reg=True, ax=ax9)

    v_13_scatter_plot = pd.concat([Y_train,Train_data['v_13']],axis = 1)
    sns.regplot(x='v_13',y = 'price',data = v_13_scatter_plot,scatter= True, fit_reg=True, ax=ax10)
```
    
In[46]:
```python
    ## 1) unique分布
    for fea in categorical_features:
        print(Train_data[fea].nunique())
```

In[47]:
```python
    categorical_features
```


In[48]:
```python
    ## 2) 类别特征箱形图可视化

    # 因为 name和 regionCode的类别太稀疏了，这里我们把不稀疏的几类画一下
    categorical_features = ['model',
    'brand',
    'bodyType',
    'fuelType',
    'gearbox',
    'notRepairedDamage']
    for c in categorical_features:
        Train_data[c] = Train_data[c].astype('category')
        if Train_data[c].isnull().any():
            Train_data[c] = Train_data[c].cat.add_categories(['MISSING'])
            Train_data[c] = Train_data[c].fillna('MISSING')

    def boxplot(x, y, **kwargs):
        sns.boxplot(x=x, y=y)
        x=plt.xticks(rotation=90)

    f = pd.melt(Train_data, id_vars=['price'], value_vars=categorical_features)
    g = sns.FacetGrid(f, col="variable",  col_wrap=2, sharex=False, sharey=False, size=5)
    g = g.map(boxplot, "value", "price")
```


In[49]:
```python
    Train_data.columns
```


In[50]:
```python
    ## 3) 类别特征的小提琴图可视化
    catg_list = categorical_features
    target = 'price'
    for catg in catg_list :
        sns.violinplot(x=catg, y=target, data=Train_data)
        plt.show()
```


In[51]:
```python
    categorical_features = ['model',
    'brand',
    'bodyType',
    'fuelType',
    'gearbox',
    'notRepairedDamage']
```


In[52]:
```python
    ## 4) 类别特征的柱形图可视化
    def bar_plot(x, y, **kwargs):
        sns.barplot(x=x, y=y)
        x=plt.xticks(rotation=90)

    f = pd.melt(Train_data, id_vars=['price'], value_vars=categorical_features)
    g = sns.FacetGrid(f, col="variable",  col_wrap=2, sharex=False, sharey=False, size=5)
    g = g.map(bar_plot, "value", "price")
```


In[53]:
```python
    ##  5) 类别特征的每个类别频数可视化(count_plot)
    def count_plot(x,  **kwargs):
        sns.countplot(x=x)
        x=plt.xticks(rotation=90)

    f = pd.melt(Train_data,  value_vars=categorical_features)
    g = sns.FacetGrid(f, col="variable",  col_wrap=2, sharex=False, sharey=False, size=5)
    g = g.map(count_plot, "value")
```


## 用pandas_profiling生成数据报告
用pandas_profiling生成一个较为全面的可视化和数据报告(较为简单、方便) 最终打开html文件即可

In[54]:
```python
    import pandas_profiling

    pfr = pandas_profiling.ProfileReport(Train_data)
    pfr.to_file("./example.html")
```


## 经验总结
所给出的EDA步骤为广为普遍的步骤，在实际的不管是工程还是比赛过程中，这只是最开始的一步，也是最基本的一步。

接下来一般要结合模型的效果以及特征工程等来分析数据的实际建模情况，根据自己的一些理解，查阅文献，对实际问题做出判断和深入的理解。

最后不断进行EDA与数据处理和挖掘，来到达更好的数据结构和分布以及较为强势相关的特征

---
数据探索在机器学习中我们一般称为EDA（Exploratory Data Analysis）：

> 是指对已有的数据（特别是调查或观察得来的原始数据）在尽量少的先验假定下进行探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方法。

数据探索有利于我们发现数据的一些特性，数据之间的关联性，对于后续的特征构建是很有帮助的。

1. 对于数据的初步分析（直接查看数据，或.sum(), .mean()，.descirbe()等统计函数）可以从：样本数量，训练集数量，是否有时间特征，是否是时许问题，特征所表示的含义（非匿名特征），特征类型（字符类似，int，float，time），特征的缺失情况（注意缺失的在数据中的表现形式，有些是空的有些是”NAN”符号等），特征的均值方差情况。

2. 分析记录某些特征值缺失占比30%以上样本的缺失处理，有助于后续的模型验证和调节，分析特征应该是填充（填充方式是什么，均值填充，0填充，众数填充等），还是舍去，还是先做样本分类用不同的特征模型去预测。

3. 对于异常值做专门的分析，分析特征异常的label是否为异常值（或者偏离均值较远或者事特殊符号）,异常值是否应该剔除，还是用正常值填充，是记录异常，还是机器本身异常等。

4. 对于Label做专门的分析，分析标签的分布情况等。

5. 进步分析可以通过对特征作图，特征和label联合做图（统计图，离散图），直观了解特征的分布情况，通过这一步也可以发现数据之中的一些异常值等，通过箱型图分析一些特征值的偏离情况，对于特征和特征联合作图，对于特征和label联合作图，分析其中的一些关联性。

参考：[Datawhale仓库参考](https://github.com/datawhalechina/team-learning/blob/master/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%EF%BC%89/Task2%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.md)

